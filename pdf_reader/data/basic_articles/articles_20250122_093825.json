[
  {
    "url": "https://www.jiqizhixin.com/articles/2025-01-21-11",
    "title": "预测精度媲美实验！哥大团队开发可解释细胞「基础」模型，揭示213种人类细胞调控语法",
    "author": "ScienceAI原创2025/01/21 19:13",
    "date": "2025/01/21 19:13",
    "content": "[图片: https://image.jiqizhixin.com/uploads/editor/c242ca1d-ef29-46de-beb6-cbd072adc8e1/640.png]\n编辑 | 萝卜皮\n转录调控涉及调控序列和蛋白质之间的复杂相互作用，指导所有生物过程。转录计算模型缺乏通用性，无法准确推断未知的细胞类型和条件。\n哥伦比亚大学的研究人员介绍了 GET（general expression transformer），这是一种可解释的基础模型，旨在揭示 213 种人类胎儿和成人细胞类型的调控语法。\nGET 完全依赖染色质可及性数据和序列信息，即使在以前未见过的细胞类型中，也能达到实验级的准确度，预测基因表达。\nGET 还在新的测序平台和检测中表现出显著的适应性，能够对广泛的细胞类型和条件进行调控推断，并揭示通用和细胞类型特异性的转录因子相互作用网络。\n该研究以「\nA foundation model of transcription across human cell types\n」为题，于 2025 年 1 月 8 日发布在《\nNature\n》。\n[图片: https://image.jiqizhixin.com/uploads/editor/25e6bdcb-82ce-4a49-8c70-dc0b9c684a56/640.png]\n「预测性可推广的计算模型可以快速准确地揭示生物过程。这些方法可以有效地进行大规模计算实验，促进和指导传统的实验方法。」系统生物学教授、论文的通讯作者 Raul Rabadan 说。\n传统的生物学研究方法擅长揭示细胞如何工作或如何对干扰作出反应。但它们无法预测细胞如何工作或细胞如何对变化作出反应，例如致癌突变。\n「能够准确预测细胞活动将改变我们对基本生物过程的理解。」Rabadan 说，「它将使生物学从一门描述看似随机的过程的科学转变为一门能够预测控制细胞行为的根本系统的科学。」\n「以前的模型都是针对特定细胞类型的数据进行训练的，通常是癌细胞系或其他与正常细胞几乎没有相似之处的细胞。」Rabadan 说。\nRabadan 实验室的研究生 Xi Fu 决定采取不同的方法，利用从正常人体组织中获得的数百万个细胞的基因表达数据来训练机器学习模型。输入包括基因组序列和显示基因组哪些部分可访问和表达的数据。\n基于这些想法，他们研发了 GET，这是一种最先进的基础模型，专门设计用于解释控制多种人类细胞类型的转录调控机制。通过整合染色质可及性数据和基因组序列信息，GET 实现了与遗漏细胞类型中的实验重复相当的预测精度水平。\n总体方法与 ChatGPT 等流行的「基础」模型的工作方式类似，使用一组训练数据来识别底层规则，即语言的语法，然后将这些推断出的规则应用于新情况。\n「这里完全相同的事情：我们在许多不同的细胞状态下学习语法，然后我们进入一种特定的状态 - 它可能是患病的[细胞类型]，也可能是正常的细胞类型 - 我们可以尝试看看我们如何根据这些信息预测模式。」Rabadan 说。\n[图片: https://image.jiqizhixin.com/uploads/editor/21270a6d-95c5-4674-a45c-072de88a7d58/640.png]\n图示：GET 模型及其应用。（来源：论文）\nGET 从 213 种人类胎儿和成人细胞类型的染色质可及性数据中学习转录调控语法，并准确预测可见和不可见细胞类型中的基因表达。\n此外，GET 提供报告基因检测读数的零样本预测，在识别顺式调控元件方面优于以前最先进的模型，并识别以前未知和已知的胎儿血红蛋白上游调节剂。\n[图片: https://image.jiqizhixin.com/uploads/editor/049c9a7f-4150-4fff-abf3-326affa97de1/640.png]\n图示：GET 通知 TF–TF 交互发现。（来源：论文）\nGET 还提供了丰富的细胞类型特异性调控见解：利用 GET 预测的共调节信息，研究人员精确定位了潜在的基序-基序相互作用，并构建了人类 TF 和辅激活因子的结构相互作用目录。\n目录链接：\nhttps://huggingface.co/spaces/get-foundation/getdemo\n利用此目录，研究人员确定了涉及 PAX5 和核受体家族 TF 的淋巴细胞特异性 TF-TF 相互作用，并强调了白血病相关生殖系变异的可能疾病驱动机制，该机制影响 PAX5 无序区域与核受体域的结合。\n当然 GET 还存在一些局限性。GET 目前的局限性包括主要依赖于染色质可及性数据、有界分辨率来区分具有非常相似基序的 TF 同源物，以及仅对粗粒度细胞状态和区域级序列信息进行训练。\nGET 未来的增强可能涉及整合多层生物信息，包括但不限于核苷酸水平的调节足迹、三维染色质结构以及调节表达谱或单细胞嵌入。\nGET 的未来迭代可以整合更多患病、受干扰或经过处理的细胞状态和更广泛的检测，包括直接测量 TF 结合、组蛋白修饰和 PolII 活性的检测，以提供对监管格局的更全面的了解。\n[图片: https://image.jiqizhixin.com/uploads/editor/94072def-4b57-470d-a466-3cb02e8512d7/640.png]\n图示：GET 识别受癌症相关种系变异影响的细胞类型特异性 TF-TF 相互作用。（来源：论文）\n多路复用核苷酸水平扰动或随机化将有助于校准 GET，以精确预测非编码遗传变异的功能影响。确定非编码变异在调节基因表达和疾病易感性方面的影响仍然是一个重要的探索领域。\n将基因组变异整合到 GET 框架中将使研究人员能够更准确地预测它们对基因调控的影响，从而深入了解复杂性状和疾病的遗传基础。\n此外，基因调控动力学反映了转录活性在发育线索或环境刺激下的时间变化，这是可以整合到模型中的另一个复杂性维度。\n借助团队高效的微调框架，使用预训练和微调的 GET 进行比较解释分析可用于识别驱动细胞状态变化的重要调节区域或基序。\n基于 GET 构建的生成模型可以开发并用于设计兆碱基级增强子阵列，并设计细胞类型特异性 TF 或其相互作用抑制剂，以进行有针对性的治疗干预。\n总的来说，GET 代表了细胞类型特异性转录建模的一种先驱方法，在调节元件、上游调节剂和 TF 相互作用的识别方面具有广泛的适用性。\n论文链接：\nhttps://www.nature.com/articles/s41586-024-08391-z\n相关报道：\nhttps://phys.org/news/2025-01-biologists-ai-cells.html",
    "tags": [
      "产业",
      "产业"
    ],
    "category": "产业",
    "images": [
      "https://image.jiqizhixin.com/uploads/editor/c242ca1d-ef29-46de-beb6-cbd072adc8e1/640.png",
      "https://image.jiqizhixin.com/uploads/editor/25e6bdcb-82ce-4a49-8c70-dc0b9c684a56/640.png",
      "https://image.jiqizhixin.com/uploads/editor/21270a6d-95c5-4674-a45c-072de88a7d58/640.png",
      "https://image.jiqizhixin.com/uploads/editor/049c9a7f-4150-4fff-abf3-326affa97de1/640.png",
      "https://image.jiqizhixin.com/uploads/editor/94072def-4b57-470d-a466-3cb02e8512d7/640.png"
    ]
  },
  {
    "url": "https://www.jiqizhixin.com/articles/2025-01-21-10",
    "title": "看破不可见数据集，自我监督学习成为细胞组学新的复杂系统处理利器",
    "author": "ScienceAI原创2025/01/21 19:11",
    "date": "2025/01/21 19:11",
    "content": "[图片: https://image.jiqizhixin.com/uploads/editor/b5d19275-be19-42e3-85ab-08836b72d6c4/640.png]\n编辑丨&\n自我监督学习 SSL 是一个概念，即数据及其固有的成对关系足以学习有意义的数据表示。监督学习依赖于成对的观察值和标签 ，而 SSL 仅依赖于输入和样本间关系 。\nSSL 已成为一种强大的方法，用于从庞大、未标记的数据集中提取有意义的表示，从而改变计算机视觉和自然语言处理。\n在单细胞基因组学 （SCG） 中，表征学习提供了对复杂生物数据的见解，尤其是新兴的基础模型。然而，在 SCG 中识别 SSL 优于传统学习方法的场景仍然是一个微妙的挑战，在 SSL 框架内为 SCG 选择最有效的借口任务是一个关键但尚未解决的问题。\n来自德国慕尼黑的一支研究团队试图通过调整和基准测试 SCG 中的 SSL 方法来解决这一差距，这其中包括具有多种掩码策略的掩码自动编码器和对比学习方法。\n他们的研究结果以「\nDelineating the effective use of self-supervised learning in single-cell genomics\n」为题，于 2024 年 12 月 27 日发布在《\nNature Machine Intelligence\n》。\n[图片: https://image.jiqizhixin.com/uploads/editor/930b2b88-9b19-4603-8f75-236f778c02c9/640.png]\n在 SCG 中，掩蔽自动编码器优于对比方法，这与计算机视觉趋势不同。SSL 在零镜头设置与跨模态预测和数据集成方面中有着显著潜力。\nSSL 在 SCG 之中\n单细胞基因组学 （SCG） 已迅速扩展到大数据领域，这主要是由单细胞 RNA 测序技术的进步引起的。更大的数据集会带来更多的挑战，而大模型便因此受到关注并急速发展。\n然而，在理解他们的用例以及如何有效利用包含数百万个单元的新兴数据集方面仍然存在差距。SCG 领域现在不仅需要计算能力，还需要战略性地使用处理大数据复杂性的方法。在这种情况下，SSL 是一种很有前途的方法。\nSSL 通常是基础模型的根本，已经开始影响到小型和大型 SCG。在小规模上，专门的 SSL 方法部署了对比损失，使用多模态学习等技术进行定制，基于图形的策略和基于聚类的方法以嵌入单元格。\n虽然基础模型已经通过自我监督的预训练展示了改进，但理清 SSL、扩展定律或 transformer 架构的贡献仍然很困难。\n为了指导 SSL 在 SCG 中的有效使用，需要通过系统的经验验证来解决这些歧义。此类研究有助于确定 SSL 可以有效促进 SCG 的场景。\n团队的研究旨在确定 SCG 中 SSL 有用的特定场景，并彻底分析和评估 SCG 中的 SSL 方法。基于 SCG 中明确定义的 SSL 基准指标，实证分析主要集中在细胞类型预测应用上，并在基因表达重建、跨模态预测和数据集成方面进行验证。\n他们发现， SSL 可以提高迁移学习设置中的下游性能，即在分析由来自较大辅助数据集的见解提供的较小数据集时，以及在涉及看不见的数据集的情况下。\nSSL 框架原意是用于开发自我监督方法并研究 SCG 中的不同用例，其核心是使用完全连接的自动编码器架构，这些架构因其在 SCG 任务中无处不在的应用而被选中。\n[图片: https://image.jiqizhixin.com/uploads/editor/27147935-f367-4503-8629-26ebf372cbee/640.png]\n图示：SCG 中辅助数据上的 SSL 提高的性能。（图源：论文）\n这些优化策略需要利用不同程度的生物学洞察力，从具有最小归纳偏差的随机掩蔽到密集利用已知基因功能的孤立掩蔽，强调有针对性的生物学关系。\nSSL 与训练后的预测\n作为 SCG 中自我监督的第一个用例，团队询问了对细胞图谱或较小数据集的分析是否可以从辅助数据的自我监督预训练中受益。\n值得注意的是，在大量供体上进行预训练，SSL 的性能优于监督学习，这凸显了丰富的预训练数据集的必要性。\n团队对 SSL 方法的基准测试揭示了对选择预训练策略的敏感性。对比学习已被证明在语言或者视觉建模等领域有效的方案，SSL 在较小规模上有效。\n[图片: https://image.jiqizhixin.com/uploads/editor/f8502daa-bef3-4473-957a-142fbf545c53/640.png]\n图示：SSL 在看不见的数据集上实现了高零样本性能与更高的准确性。（图源：论文）\n如果为监督模型和 SSL 模型提供对相同数据的访问权限，它们的性能将非常相似。倘若把这点扩展到看不见的数据集中，就能发现，虽然都是在分布内部，但是在分析看不见的数据集时，SSL 对于泛化的运用更加具有优势。\n在对 SSL 在转录组学上的效用进行了基准测试后，研究团队试图将研究扩展到多组学，意在寻找 SSL 是否可以利用来自一种模态的辅助数据来增强多模态下游任务。\n在经历了对蛋白质组学计数等预训练后，团队得出了结论。SSL 在预测上的性能明显优于其监督对应物。这一发现突出了在一种模式更丰富的情况下自我监督的优势。\n更多的发展方向\n由于批次效应（例如实验条件或混杂因素），集成单细胞数据集进行联合分析非常困难，这给图谱分析工作带来了独特的挑战。\n团队的实验结果阐明了 SSL 可以表现出色的背景，尤其是在利用来自庞大辅助数据集的见解进行较小的数据集任务和看不见的数据集场景时。\nSSL 与受监督方法相同，在监督方法中，两者都访问相同的数据，并且零样本 SSL 模型接近该性能。\n团队为 SCG 中的 SSL 提供了稳健的、以实证为基础的观点，为研究复杂生物系统提供更明智的数据驱动方法铺平了道路。在大型模型与基础模型的上下文中，这些理解可以帮助设计预训练和选择借口任务。\nSSL 方法的基准为从业者提供了关于在上述设置中哪种方法有利的明确建议。因其在各种任务中具有鲁棒性和多功能性，团队建议使用随机掩码策略进行掩码预训练，这是基础模型的核心。\n对于更广泛的计算生物学社区，研究团队已经证明，对图谱级数据进行自我监督的预训练有助于提高通常更难扩展的生物学或医学相关性较小数据集的性能。\n原文链接：\nhttps://www.nature.com/articles/s42256-024-00934-3\n代码链接：\nhttps://doi.org/10.5281/zenodo.13358872",
    "tags": [
      "理论",
      "理论"
    ],
    "category": "理论",
    "images": [
      "https://image.jiqizhixin.com/uploads/editor/b5d19275-be19-42e3-85ab-08836b72d6c4/640.png",
      "https://image.jiqizhixin.com/uploads/editor/930b2b88-9b19-4603-8f75-236f778c02c9/640.png",
      "https://image.jiqizhixin.com/uploads/editor/27147935-f367-4503-8629-26ebf372cbee/640.png",
      "https://image.jiqizhixin.com/uploads/editor/f8502daa-bef3-4473-957a-142fbf545c53/640.png"
    ]
  },
  {
    "url": "https://www.jiqizhixin.com/articles/2025-01-21-9",
    "title": "原生融合多模态上的突破，让商汤大模型打破Scaling Laws撞墙「魔咒」",
    "author": "机器之心原创2025/01/21 17:17",
    "date": "2025/01/21 17:17",
    "content": "基础模型的革新，才是通向未来之路。\n下一代 AI 的发展，似乎遇到了难以逾越的瓶颈。\n去年 12 月，OpenAI 在 ChatGPT 两周年期间连续发布了 12 天，我们期待的新一代大模型 GPT-5 却从头到尾没有踪影。\n失望之后，随之而来的还有各路媒体的报道——各大人工智能实验室似乎同时在大型语言模型竞赛中撞了墙。\n[图片: https://image.jiqizhixin.com/uploads/editor/257a880d-8c58-4b6e-8dad-61569192a1dd/640.png]\nOpenAI 的「GPT-5」内部代号 Orion，已经进行了为期数月的后期训练，然而该模型发布经历了多次延迟。知情人士表示，Orion 至今仍未达到可发布水平，OpenAI 不太可能在最近推出该系统。与此同时，Anthropic 等其他公司的下一代模型也面临着同样的问题。\n大型模型的训练可能需要花费数千万美元。由于系统的复杂性，模型的训练可能需要数月时间，除了 GPU 的需求暴增，甚至电力也成为了阻碍 AI 训练进行的瓶颈。数据是大模型面临的又一大挑战，生成式 AI 发展至今，我们距离耗尽全球所有可访问数据已经越来越近了。\n为了克服这些挑战，研究人员正在把目光转向新的方向。\n「2010 年代是扩展的时代，现在我们又回到了好奇与发现的时代。每个人都在寻找下一个目标，」OpenAI 前首席科学家 Ilya Sutskever 表示。「现在，找到正确的扩展方向比以往任何时候都更加重要。」\n生成式 AI 的下个形态\n正在浮出水面\n其实，我们对 AI 的下个大方向并非毫无头绪。\n[图片: https://image.jiqizhixin.com/uploads/editor/d1afff11-6b28-4f49-be25-f245c8b62dcb/640.gif]\n2024 年 8 月，谷歌实验版的 Gemini 1.5 Pro 超越了 GPT-4o，宣告了大模型竞赛「逆袭」成功，如今不论是在消费端还在 AI 社区，人们都认为谷歌提出的技术最具颠覆性，已经重回到了领先梯队。\n面对新一轮理论升级，Anthropic 等公司迅速跟进，OpenAI 则拿出了主打「复杂推理」的 o1 大模型，旨在专门解决难题。\n国内企业也投身于新道路的探索。近日，\n商汤科技实现了原生融合模态训练上的实质性突破，发布了「日日新」融合大模型\n。\n生成式 AI 爆发后，多模态大模型早已成为人们追求的方向。然而，我们在很多应用中接触到的多模态模型并不能说是「完全体」。\n模态融合（Multimodal Fusion）被认为是 AI 未来发展的必由之路。\n就像谷歌所认为的，只有从头开始的多模态才能构建出超越前代的先进模型。这意味着它天生地可以读取和输出不同模态内容，还具备强大的多模态推理能力和跨模态迁移能力。\n[图片: https://image.jiqizhixin.com/uploads/editor/d170b7dd-7bb2-4693-9dd5-cf18260314b7/640.png]\n图片来源：https://arxiv.org/abs/2312.11805\n这是一个符合直觉的技术方向——只有让机器拥有对物理世界中多模态、多维度信息的感知，拥有了综合的理解，它们才能发展出类似于人类的分析、判断、正确决策能力。\n在新范式下，你可以自然地与 AI 进行交流：发一段语音、添加一张图片、输入一些文本，甚至直接录短视频都行；同样的，输出也是自然的多模态形式。\n商汤原生融合的多模态模型，打破了一直以来大语言模型、多模态大模型分立的行业局面，真正意义上迈向了模型一统。\n对行业来说，大模型进入了多模态时代。随着走向通用和一体化，并在视觉、语音、数学推理等方面实现了前所未有的能力，一线大模型的技术门槛将大幅拉高。\n抢先实测\n「原生融合多模态」优势尽显\n得益于在计算机视觉领域超过十年深耕和丰富经验，进入多模态时代之后，商汤的独有优势正在逐渐显现。\n日前，商汤还对外发布了\n「日日新」融合大模型交互版（SenseNova-5o）\n，它基于「日日新」融合大模型的能力，\n提供实时音视频对话服务\n，我们也立刻下载进行了测试。\n为了测试它的反应和理解能力，我们举着手机在编辑部开启「夺命连环 call」。\n[图片: https://image.jiqizhixin.com/uploads/editor/64a01a61-1b7e-429e-93a3-2765b36bb748/1737450806249.png]\n[图片: https://image.jiqizhixin.com/uploads/editor/d9eefb81-74e4-4030-bf25-0997664760b8/1737450816191.png]\n简单测试下来，我们发现它的反应速度很快，与真人对话无异，并且可以随时打断和接话。而且，SenseNova-5o 还拥有令人满意的记忆力，可以长达 5 分钟，因此它能在多轮对话中持续不断理解使用者需求，并且准确记住几分钟之前，曾经听到、看到的内容。\n这意味着多模态的 AI 已经可以拓展出一些新的应用场景，比如帮助孩子解读题目，给出清晰的解读思路。\n[图片: https://image.jiqizhixin.com/uploads/editor/27ac52fc-3d04-4c2b-a752-ee51992d5a0c/1737450830213.png]\n充分支持实现音频、图像、视频的任意组合的多模态输入，以及自然流畅的语音内容输出，商汤走出了迈向更自然人机交互的新一步。\n体验了交互能力之后，我们还在商汤「商量」网页版中，测试了全国首个原生融合多模态大模型——商汤「日日新」融合大模型更加全面的表现。\n搞笑搭子\n最近一大波外国人疯狂涌入小红书，为了拉近与中国网友的关系，他们主动交猫税、开班教英语、手把手辅导作业……\n更搞笑的是，评论区还被龙妈和唐僧的同框照刷了屏。\n我们把该图丢给商量，它不仅认出两个影视人物，还读懂了这张图背后表达的跨文化传播的幽默感。\n[图片: https://image.jiqizhixin.com/uploads/editor/7a436681-d8cb-4df2-8a4a-8c44855e7d14/640.png]\n再比如这张恶搞电影《华尔街之狼》的梗图。\nAI 先分别描述了图片上下两部分的场景，然后揣摩出其中的「深意」——只要将 AI 元素融入日常物品中，就能提升其价值——一语中的。\n[图片: https://image.jiqizhixin.com/uploads/editor/e5ae3f47-cb2c-4a57-9586-f67f29cc069b/640.png]\n当被问到「这个场景来自哪部电影？」时，商汤「日日新」一口答出《华尔街之狼》，还简单介绍了其基本信息。\n[图片: https://image.jiqizhixin.com/uploads/editor/74a386b2-abcc-4201-80fd-d98f50b39cf7/640.png]\n旅游搭子\n它还是逛博物馆的好「搭子」。\n只需随手一拍，它就能把文物的「前世今生」捋一遍。\n就比如这顶明孝端皇后的「九龙九凤冠」，其精美程度让人叹为观止。仅用一张图片，商量就能扒出它的尺寸、设计以及制作工艺等。\n[图片: https://image.jiqizhixin.com/uploads/editor/f1ab286c-dcef-48ab-b8a7-b00d2dcf1ac2/640.png]\n学习搭子\n测试多模态大模型的逻辑推理能力，自然少不了数学题。今年深圳南山区数学题难倒一片小学生，我们从中选取一道来考考商汤「日日新」。\n它对着题目就是一顿分析，在给出正确答案的同时，还列出了解题思路。\n[图片: https://image.jiqizhixin.com/uploads/editor/fe89824b-1d1f-4bc1-914f-a42511591305/640.png]\n对于小红书上中外网友探讨的数学作业，商汤「日日新」也能分析得头头是道。\n[图片: https://image.jiqizhixin.com/uploads/editor/8d22f730-7895-44e9-b4cd-1e79d19a4f6e/640.png]\n此外，它还能进行图表分析。\n从概念理解，到折线图中关键要素提取，再到信息分析，AI 的「大脑」在高速运转，几个步骤合一迅速完成。\n[图片: https://image.jiqizhixin.com/uploads/editor/5d1f5dba-0544-47f4-ae81-09b23eae4ccf/640.png]\n更低成本\n已商业落地\n目前，商汤「日日新」融合大模型已向客户开放了端到端 API 调用，同时融合大模型交互版（SenseNova-5o）也已经面向视觉交互场景开放商用（限时免费！）。\n其中，针对商用版本的 SenseNova-5o，商汤将提供两种交互模式的服务。\n[图片: https://image.jiqizhixin.com/uploads/editor/fa610677-632f-49d1-99bb-238cfa013250/1737450855616.png]\n半双工模式：类似对讲机模式，双方交替发言，可以支持平均 560 毫秒响应音频与图像输入，与人类的对话交互的响应接近，同时支持 1200×800px 的图像解析，不超过 30 秒的音频输入，不超过 720p 的视频输入。\n全双工模式：类似电话的通信模式，AI 可以实时理解用户意图并生成回应，实现流畅自然的语音 + 视频交互，实现了接近人类面对面交流的体验。\n[图片: https://image.jiqizhixin.com/uploads/editor/70330c65-c801-42b9-8377-574c6ceb9f88/640.png]\nSenseNova-5o 基础架构\n而且根据最新权威测评，商汤基于原生融合的多模态大模型 ——「日日新」融合大模型，在图文推理、语言等各方面都达到了业内最优水平。\n[图片: https://image.jiqizhixin.com/uploads/editor/5ff42c5f-a390-403e-91da-dec1fecded68/640.png]\n在 SuperCLUE 最新的《中文大模型基准测评 2024 年度报告》中，商汤「日日新」和 DeepSeek V3 并列总榜国内第一。在权威综合评测权威平台 OpenCompass 的多模态评测中，商汤「日日新」也取得了第一名，成绩领先 GPT-4o、Claude 3.5 Sonnet 等。\n这也让我们发现，采用了原生融合模态训练的多模态大模型的每一种单模态能力，都超越了只在单模态数据上训练的模型的性能 —— 它们在不同模态的数据学习中，\n涌现出在多模态信息上的深度推理能力，和跨模态的交互能力，显著超越了通过传统图文对齐方法的多模态模型\n。\n在预训练阶段，商汤的工程师不仅使用了天然存在的海量图文交错数据，还通过逆渲染、基于混合语义的图像生成等方法合成了大量融合模态数据，使得模型基座对于模态之间的关系有更扎实的掌握，为更好地完成跨模态任务打下基础。\n在后训练阶段，基于对广泛业务场景的认知，商汤构建了大量的跨模态任务，包括视频交互、多模态文档分析、城市场景理解、车载场景理解等。通过把这些任务融入到增强训练的过程，商汤的融合模态模型获得了强大的多模态理解分析能力，对大量业务场景能够形成有效响应。\n而且商汤表示，和分别训练一个语言大模型、一个多模态模型相比，训练商汤「日日新」融合大模型的总体成本反而降低了 40%。\nAI 扩展定律\n还有几个数量级的空间\n中国正在 AI 领域快速发展，有赶超美国的趋势。这是谷歌前 CEO 埃里克・施密特（Eric Schmidt）表示最近发表的看法，他给出的理由是：中国正在把 AI 技术快速应用于大规模生产。\n国内庞大产业体系和需求，正在逐渐成为驱动 AI 发展的决定性力量。\n深耕人工智能技术落地多年的商汤，在模型算法、算力、行业经验、工程落地能力等方面，都具备了绝对的优势。据了解，商汤「日日新」融合大模型，和融合大模型交互版（SenseNova-5o）已经落地在具身机器人、AI 眼镜、手机、教育等场景。\n商汤科技联合创始人、人工智能基础设施及大模型首席科学家林达华表示：「多模态大模型应该与广泛的业务场景相结合，能够在真实场景中去解决一些复杂的问题，完成复杂的任务。在交互场景，如人与人对话的过程中，多模态能力可以做到很多以往做不到的事。」\n去年 12 月，在全球 AI 顶级学术会议 NeurIPS 上，Ilya Sutskever 发表演讲对于人工智能可用数据枯竭表示了担忧，让人们对 Scaling Laws 是否终结的大讨论愈演愈烈。\n对于大模型的 Scaling Laws，商汤也给出了自己的判断。林达华表示，当前利用互联网数据进行预训练的方法，确实很快就会到达瓶颈。\n但真实世界的数据并不仅限于互联网：工作时的 OA 流程，汽车驾驶时传感器记录的状态，科学研究时获得的数据等等，这些内容会比文字形式存在于互联网上的数据多出四到五个数量级。\n想要利用好真实世界中的数据，就必须构建起结合多模态的 AI 模型，这就是商汤坚定投身多模态新方向的原因。\n换言之，大模型早已不局限于「做题」了。商汤走通了原生融合模态的技术路径之后，未来已经出现了前所未有的想象空间。甚至在图像 + 文字输入之后，我们还可以期待整个空间结构的输入、机器人与 LLM 推理能力的高度结合，还有很多领域值得去拓展。\n传送门：\nSenseNova-5o 正式接口及接入方案：https://sensenova5o_doc.sensetime.com/introduction/intro.html",
    "tags": [
      "产业SenseNova-5o商汤科技",
      "产业",
      "SenseNova-5o",
      "商汤科技"
    ],
    "category": "产业",
    "images": [
      "https://image.jiqizhixin.com/uploads/editor/257a880d-8c58-4b6e-8dad-61569192a1dd/640.png",
      "https://image.jiqizhixin.com/uploads/editor/d1afff11-6b28-4f49-be25-f245c8b62dcb/640.gif",
      "https://image.jiqizhixin.com/uploads/editor/d170b7dd-7bb2-4693-9dd5-cf18260314b7/640.png",
      "https://image.jiqizhixin.com/uploads/editor/64a01a61-1b7e-429e-93a3-2765b36bb748/1737450806249.png",
      "https://image.jiqizhixin.com/uploads/editor/d9eefb81-74e4-4030-bf25-0997664760b8/1737450816191.png",
      "https://image.jiqizhixin.com/uploads/editor/27ac52fc-3d04-4c2b-a752-ee51992d5a0c/1737450830213.png",
      "https://image.jiqizhixin.com/uploads/editor/7a436681-d8cb-4df2-8a4a-8c44855e7d14/640.png",
      "https://image.jiqizhixin.com/uploads/editor/e5ae3f47-cb2c-4a57-9586-f67f29cc069b/640.png",
      "https://image.jiqizhixin.com/uploads/editor/74a386b2-abcc-4201-80fd-d98f50b39cf7/640.png",
      "https://image.jiqizhixin.com/uploads/editor/f1ab286c-dcef-48ab-b8a7-b00d2dcf1ac2/640.png",
      "https://image.jiqizhixin.com/uploads/editor/fe89824b-1d1f-4bc1-914f-a42511591305/640.png",
      "https://image.jiqizhixin.com/uploads/editor/8d22f730-7895-44e9-b4cd-1e79d19a4f6e/640.png",
      "https://image.jiqizhixin.com/uploads/editor/5d1f5dba-0544-47f4-ae81-09b23eae4ccf/640.png",
      "https://image.jiqizhixin.com/uploads/editor/fa610677-632f-49d1-99bb-238cfa013250/1737450855616.png",
      "https://image.jiqizhixin.com/uploads/editor/70330c65-c801-42b9-8377-574c6ceb9f88/640.png",
      "https://image.jiqizhixin.com/uploads/editor/5ff42c5f-a390-403e-91da-dec1fecded68/640.png"
    ]
  },
  {
    "url": "https://www.jiqizhixin.com/articles/2025-01-21-8",
    "title": "选择/杂交/突变，DeepMind将自然选择引入LLM思维，实现心智进化",
    "author": "机器之心原创2025/01/21 17:12",
    "date": "2025/01/21 17:12",
    "content": "今天是个好日子，DeepSeek 与 Kimi 都更新了最新版的推理模型，吸引了广泛关注。与此同时，谷歌 DeepMind、加州大学圣地亚哥分校、阿尔伯塔大学的一篇新的研究论文也吸引了不少眼球，并直接冲上了 Hugging Face 每日论文榜第一（1 月 20 日）。\n[图片: https://image.jiqizhixin.com/uploads/editor/3c17314c-fb03-49f8-a731-8490d88e3855/640.png]\n这篇论文题为《Evolving Deeper LLM Thinking》，可译为「进化式更深度 LLM 思维」，其中提出了一种进化搜索策略，可用于 scaling LLM 的推理时计算（inference time compute）。该方法被命名为 Mind Evolution，即心智进化。实验表明，在同等推理成本下，新方法的自然语言规划任务表现会显著优于 Best-of-N 和 Sequential Revision 等其它推理策略。\n[图片: https://image.jiqizhixin.com/uploads/editor/02abc9a6-8518-4cfd-8cf4-b2aa4986028a/640.png]\n论文地址：https://arxiv.org/pdf/2501.09891\n如何实现心智进化\nMind Evolution 采用了遗传搜索策略，并结合了一个 LLM 和定制的提示集，从而可以有效地搜索自然语言规划任务的解。为了理解 Mind Evolution，我们首先需要简单了解基于语言的遗传算法。\n基于语言的遗传算法\n遗传算法是一种受自然选择启发的元启发式算法。在遗传算法中，候选解种群会朝着包含更多高质量个体的种群方向演化，这里的质量是相对于目标优化目标而言的。这个目标通常也被称为「适应度」函数。每个候选个体都有一个可以突变并与其他个体重组的遗传表示。\n演化搜索通常始于独立生成的候选解种群。在每一代中，都会根据目标评估每个个体的适应度。然后基于适应度对候选个体进行随机选择（「选择」）。在繁殖过程中，被选择的父代的遗传表示会进行组合（「杂交」）并可能发生改变（「突变」）以产生新的子代解。这个过程创造了下一代的子代，它们随后进入种群。由于适应度更高的父代更有可能被选择进行重组，种群适应度通常会随着连续几代而提高。\n岛屿模型。为了维持演化种群的多样性，还可引入岛屿模型。在该模型中，不同的子种群（「岛屿」）会独立演化，直到按照特定频率发生「迁移」和「岛屿重置」事件。对于迁移操作，一个岛屿上的解会基于适应度被随机选择迁移到相邻岛屿。对于岛屿重置操作，整体适应度较低的岛屿上的种群会被全局种群中的强解替换，这也具有选择效应。最近已经有一些研究成功采用了岛屿模型，如 FunSearch。\n基于语言的遗传表示。基于语言的遗传算法中的个体候选解由自然语言表示。这允许通过提示词来利用 LLM 强大的语言理解和生成能力来实现强大的重组（杂交和突变）和岛屿重置操作。\nMind Evolution\nMind Evolution 的设计见图 1，其超参数则见表 1。\n[图片: https://image.jiqizhixin.com/uploads/editor/e424462d-b811-4784-ab73-181441a340be/640.png]\n[图片: https://image.jiqizhixin.com/uploads/editor/cc4e4b3b-97d9-4146-a983-4eee6531ba47/640.png]\nMind Evolution 的核心组件包括：\n选择和迁移操作的具体选择；\n一个提示集，可使用 LLM 实现初始化、重组（杂交和突变）以及岛屿重置操作；\n一个适应度函数，用于评估给定解的质量并可选择性地反馈检测到的问题。\n整个演化过程会重复进行，直到找到有效解，或者直到完成 N_gens 代演化，之后返回得分最高的候选解。\n适应度评估。该团队为每个问题域实现了一个适应度函数，其中候选解会被解析并以编程方式进行评估。原则上，任何可以评估解质量的函数都可以使用，包括 LLM 评估。\n在 Mind Evolution 中，评估函数有三个关键作用：\n通过衡量优化目标为解评分（如果有的话）；\n验证解是否满足给定约束；\n提供相应的文本反馈。\n需要注意的是，对于许多经典搜索问题（如 NP 完全问题），验证解比解决问题要容易得多。同样，该该团队观察到，对于所考虑的自然语言规划任务，编写评估函数是可能的。能够检查候选解的正确性并不意味着能在这个任务找到有效解。也就是说，实现评估函数并不等同于解决任务。\n种群初始化。给定目标问题，通过向 LLM 提供问题描述、解决问题所需的任何信息以及相关指令，独立采样 N_convs 个初始解。如果 N_seq > 1，则每个初始解都会通过「通过批评性对话进行优化（Refinement through Critical Conversation）」过程的 N_seq - 1 个额外轮次进行评估和改进，该过程将在下文解释。\n这个初始化过程一共会生成 N_convs × N_seq 个候选解，它们构成了第一代第一个岛屿上的初始种群。\n通过批评性对话进行优化（RCC）。给定一个候选解（或用于重组过程的一组候选解），该团队利用 LLM 通过组织「批评者」角色和「作者」角色之间的批评性对话来生成改进的解，如图 2 所示。\n[图片: https://image.jiqizhixin.com/uploads/editor/af2dd6f0-a605-4bab-9366-602ab1ff6e28/640.png]\n分离这两个角色的目标是提高 LLM 的批判性思维能力。每轮对话都会被构建为一个由提示词驱动的过程，其中解会根据批评性反馈进行改进，类似于 Reflexion。\n具体来说，批评者首先会分析输入的候选解，解读文本评估反馈，并建议纠正反馈中提到的问题的方法。然后，作者基于输入候选解、后续评估和批评者的分析提出一个改进的解。\n选择。为了产生岛屿的下一代，该团队遵循玻尔兹曼锦标赛选择（Boltzmann tournament selection）方法，其中根据从适应度分数的 softmax 变换得到的概率分布，从种群中随机采样 0 到 N_parent 个父代。通过这种方式，表现更好的解更有可能被选择用于繁殖，而其他候选解仍然可以偶尔被选择以保持多样性。\n杂交和突变。该团队将杂交和突变操作实现为单个重组步骤，即指示 LLM 使用上述 RCC 过程来改进给定的一组父代（图 2）。具体来说，对于重组，采样 1 到 N_parent 个父代，并修改图 2 中的步骤（b）以首先纳入父代的评估结果，然后对所有父代应用批评者并将修改后的解作为下一代的「初始解」提出。然后，如果 N_seq > 1，继续遵循步骤（c）（d）（e）顺序生成 N_seq - 1 个子代解，通过使用 RCC 过程改进每个先前的子代。\n对于每个岛屿上的每一代，都会将 N_convs × N_seq 个子代解添加到岛屿种群中，并移除重复的解。对于选择，该团队遵循玻尔兹曼锦标赛而不是显式地淘汰候选解，除非执行如下的岛屿重置。\n岛屿间迁移。在迁移事件之间，每个岛屿种群独立演化。在迁移期间，在完成当前岛屿上的这一代后，顶部的 N_emigrate 个解从当前岛屿 i 克隆到下一个岛屿 i + 1（该团队按从 1 到 N_island 的顺序顺序更新岛屿上的种群）。迁移在岛屿之间循环进行，所以从岛屿 N_island 的移民会到达岛屿 1。该团队发现这种形式的循环迁移可加速整体演化过程。\n岛屿重置。岛屿重置每隔 N_reset 代就发生一次。在岛屿重置事件期间，首先从全局种群中选择表现最好的个体，平均得分最低的 N_reset 个岛屿上的种群被淘汰，选定的表现最好的个体被克隆到重置的岛屿上。为了选择表现最好的个体，该团队探索了两种方法：\n根据适应度直接选择排名前 N_top 的候选解；\n首先根据适应度选择排名前 N_candidate 的候选解，然后提示 LLM 从这个池中选择 N_top 个彼此有实质性差异的好候选解。消融研究表明，后一种策略的效果更好。\n心智进化的实验表现\n任务。该团队在三个基准自然语言规划领域上评估了 Mind Evolution，其中包括来自 Natural Plan 的两个任务（Trip Planning 和 Meeting Planning ），以及 TravelPlanner 基准。\n模型。在实验中，该团队使用的默认 LLM 是 Gemini 1.5 Flash（gemini-1.5-flash001）。表 1 给出了将 Mind Evolution 应用于 Flash 时使用的超参数。除了评估使用 Flash 模型的 Mind Evolution 外，该团队还研究了一种两阶段方法，其中对于在 N_gens 代限制内未解决的问题使用 Gemini 1.5 Pro 模型（gemini-1.5-pro-exp-0827）。这种两阶段方法比在每个问题实例上都使用 Pro 模型更具成本效益。\n对比基线。对于每个任务，Mind Evolution 都与三种基线搜索策略进行了比较，这些策略使用了相同的解评估器和特定任务的提示词：\n1-Pass，其中使用 LLM 的单次前向传递得到解。\nBest-of-N，独立生成最多 800 个候选解，直到找到成功的解（与 Mind Evolution 上限相同）。\nSequential-Revision+，其中独立提出 10 个候选解，然后使用 RCC 过程分别修改 80 轮。注意使用 10 个独立的 80 轮改进线程而不是单个 800 轮改进，因为该团队表示很少能观察到 80 轮后的改进。这个基准方法类似于运行 10 次多轮 Reflexion。\n此外，作为参考，该团队还在对比中加入了使用 OpenAI o1-preview 的 1-Pass 基准。\nTravelPlanner\nTravelPlanner 是一个自然语言规划基准，它模拟的问题是：根据用户给出的偏好和约束条件，为用户组织旅行计划。\n表 2 比较了 Mind Evolution 与基线策略的总体成功率和计算成本。\n[图片: https://image.jiqizhixin.com/uploads/editor/4bd1fb69-30d2-434a-9b27-ac84d52d1a26/640.png]\n可以看到，在成功率方面，Mind Evolution 明显优于基线策略，超过 95%。相比之下，Sequential-Revision+ 的表现也还行，接近 83%，而 Best-of-N 逊色多了，仅有 55.6%。总的来说，进化策略的优势得到了明显体现。\n再来看看上面的两阶段方法，即使用 Gemini 1.5 Pro 处理未被解决的问题，该团队发现几乎整个数据集都可以被解决 —— 在验证和测试问题上分别达到 100% 和 99.9% 的成功率。\n该团队表示，唯一接近这个成功率的研究成果是《Large language models can plan your travels rigorously with formal verification tools》（arXiv:2404.11891）—— 该方法使用 GPT-4 进行自动形式化，然后利用形式求解器分别在验证和测试集上达到 98.9% 和 97.0% 的成功率。相较之下，Mind Evolution 完全无需形式求解器。\n最后需要注意的是，TravelPlanner 数据集包含三个难度级别（简单、中等、困难）和三个旅行时长（3 天、5 天、7 天），这就形成了 9 个不同的问题类别。图 3 展示了在这些不同类别上的成功率的细分情况。\n[图片: https://image.jiqizhixin.com/uploads/editor/df88d6a3-c5e6-42c9-9580-74d92cf6f858/640.png]\n可以看到 1-Pass 和 Best-of-N 的成功率会在规划更多旅行天数时下降，但对于 Mind Evolution 和 Sequential-Revision+ 这种迭代改进方法，这种趋势不太明显。\nNatural Plan – Trip Planning\nTrip Planning 任务的目标是找到一个行程安排，其中包含要访问的城市序列以及在每个城市停留的天数，需要满足航班连接性和日程安排约束。表 3 给出了一些问题实例。该团队将基准数据集分为了 320 个验证和 1280 个测试实例。\n[图片: https://image.jiqizhixin.com/uploads/editor/9f8bdbda-4b64-4d1f-9add-ef911a1a51da/640.png]\n同样，从表 2 可以看到，Mind Evolution 在这个任务上明显优于基线方法，其成功率在验证集上达到 96.2%，在测试实例上达到 94.1%。\n值得注意的是，Best-of-N（77.2%）在这个任务上超过了 Sequential-Revision+（74.4%）。\n该团队发现，对于两阶段方法，Mind Evolution 在验证集上的成功率达到了 100%，在测试集上也达到 99.6%。这些发现再次突出了进化搜索相对于简单采样和顺序改进的优势。\n最后需要指出，这个任务的难度会随要访问的城市数量而变化，范围从 3 到 10 个城市。图 4 显示了按城市数量划分的成功率细分情况，看起来 Mind Evolution 的相对优势随着城市数量的增加而增加。\n[图片: https://image.jiqizhixin.com/uploads/editor/6e6d4f38-44dd-48c9-8fd7-7fe826933d9f/640.png]\nNatural Plan – Meeting Planning\nMeeting Planning 的任务目标是安排一系列会议以最大化个人之间的会议数量，所涉及的限制条件包括可用性、位置和交通时间。这个任务与 TravelPlanner 和  Trip Planning  的不同之处在于，并非每个问题实例的每个会议都可安排，这意味着无法知道是否已达到最优解。因此，该团队允许搜索继续进行直到达到迭代次数的上限，最终得到了表 2 中的结果。对于这个任务，该团队将实例集分为了 500 个验证和 500 个测试实例。\n从表 2 可以看到，Mind Evolution 在验证集上达到 85.0% 的成功率，在测试集上达到 83.8%。值得注意的是，使用 Gemini 1.5 Pro 的两阶段方法在验证和测试上的成功率分别为 98.4% 和 98.2%。\n最后，图 5 显示了按需要安排会议的人数划分的成功率细分情况。该团队发现，随着人数增加，Mind Evolution 可保持显著的成功率优势。\n[图片: https://image.jiqizhixin.com/uploads/editor/58e8c139-6b90-441f-8aeb-de90c0edf39e/640.png]\n实验结果分析\n为了理解 Mind Evolution 的 scaling 性能，该团队还进行了更多研究。\nscaling 性能。图 6 报告了 Mind Evolution 在规划任务中随着代数增加的成功率变化情况。这些结果清楚地表明， Mind Evolution 会随着代数增加而稳步提升。\n[图片: https://image.jiqizhixin.com/uploads/editor/33c5f674-6c4e-40b2-8515-45bb26ccfc39/640.png]\n为了比较 Mind Evolution 与基线搜索方法的 scaling 性能，该团队还做了每种策略生成的候选解数量与成功率和平均任务评估分数的关系图（图 7-9）。任务评估分数通过对未满足的约束和目标值的次优性进行惩罚来计算，因此在任何问题实例中可以达到的最高分数是零。\n[图片: https://image.jiqizhixin.com/uploads/editor/009fa9aa-e54d-4c5c-83a0-47ffa379ec3a/640.png]\n图 7-9 分别显示了在 TravelPlanner、Trip Planning 和 Meeting Planning 任务上的结果。在每种情况下，都可以看到所有搜索方法的整体成功率和平均任务评估分数都会随着提出的解数量的增加而单调改善。这些图还表明，就达到指定成功率水平（或平均任务性能）所需的候选解数量而言，Mind Evolution 始终比基线策略更有效。\n[图片: https://image.jiqizhixin.com/uploads/editor/0007b851-a2c3-49f6-b382-96d63b74f957/640.png]\n该团队注意到 Best-of-N 在 TravelPlanner 上的表现明显不佳。该团队认为这是因为该任务涉及隐含的常识约束（例如，旅行计划应该返回出发城市，不能两次访问同一餐厅等），这些约束不在问题实例中给出，而是从评估反馈中学习得到，而 Best-of-N 没有利用这些反馈。\n该团队还进行了一系列消融研究，以研究 Mind Evolution 不同组件的效果，具体详情请参阅原论文。\n一个高难度新任务：StegPoet\n最后，在这篇论文中，该团队还提出了一个具有挑战性的新任务 StegPoet，其中需要将隐藏消息通过隐写术编码到一篇创意写作文章中。\n即使这个问题难以形式化，它仍然适合程序化验证，这使得本文考虑的方法可以处理它。\n在这个任务中，由数字序列表示的隐藏消息（M）应该被编码在关于特定主题的创意文本中，以散文、故事或诗歌的形式表达。目标是既提供一个数字到单词的替换密码，又生成使用该密码编码消息的文本。\n图 10 给出了一个例子。该团队额外施加了一个约束，即在生成的文本中，连续密码词之间必须平均有 B 个单词，这确保当 B > 0 时，简单地将密码词作为文本部分列出不符合作为解的资格。\n[图片: https://image.jiqizhixin.com/uploads/editor/226be799-61cf-4698-9ba1-72c7437654dc/640.png]\n这个问题的难度在四个维度上变化：\n随着隐藏消息 M 的长度增加，难度增加。该团队设定 10 ≤ |M| ≤ 30。\nM 中数字的重复性。重复越多，约束越严格。\n重复数字彼此之间的「接近程度」。每种写作形式都规定了同一个词的重复和出现接近程度的可接受性。LLM 必须在遵守形式和正确编码消息的需求之间取得平衡。\n根据经验，随着 B（密码词之间的平均距离）增加，问题变得更加困难。测试中，3 ≤ B ≤ 7。\n该团队将问题实例分为了 101 个验证实例和 245 个测试实例。表 6 给出了 Mind Evolution 和基线策略的详细性能结果，而图 11 显示了每个难度级别的性能。\n[图片: https://image.jiqizhixin.com/uploads/editor/e2784827-763e-4ff3-8a4d-0ad69308d4c2/640.png]\n[图片: https://image.jiqizhixin.com/uploads/editor/b97a3edf-f80f-414a-9e02-1fce91338871/640.png]\n可以看到，两阶段 Mind Evolution（+pro）在验证集上达到 87.1% 的成功率，在测试集上达到 79.2%。相较之下，Best-of-N 仅能解决 1% 的验证任务。",
    "tags": [
      "理论DeepMind",
      "理论",
      "DeepMind"
    ],
    "category": "理论",
    "images": [
      "https://image.jiqizhixin.com/uploads/editor/3c17314c-fb03-49f8-a731-8490d88e3855/640.png",
      "https://image.jiqizhixin.com/uploads/editor/02abc9a6-8518-4cfd-8cf4-b2aa4986028a/640.png",
      "https://image.jiqizhixin.com/uploads/editor/e424462d-b811-4784-ab73-181441a340be/640.png",
      "https://image.jiqizhixin.com/uploads/editor/cc4e4b3b-97d9-4146-a983-4eee6531ba47/640.png",
      "https://image.jiqizhixin.com/uploads/editor/af2dd6f0-a605-4bab-9366-602ab1ff6e28/640.png",
      "https://image.jiqizhixin.com/uploads/editor/4bd1fb69-30d2-434a-9b27-ac84d52d1a26/640.png",
      "https://image.jiqizhixin.com/uploads/editor/df88d6a3-c5e6-42c9-9580-74d92cf6f858/640.png",
      "https://image.jiqizhixin.com/uploads/editor/9f8bdbda-4b64-4d1f-9add-ef911a1a51da/640.png",
      "https://image.jiqizhixin.com/uploads/editor/6e6d4f38-44dd-48c9-8fd7-7fe826933d9f/640.png",
      "https://image.jiqizhixin.com/uploads/editor/58e8c139-6b90-441f-8aeb-de90c0edf39e/640.png",
      "https://image.jiqizhixin.com/uploads/editor/33c5f674-6c4e-40b2-8515-45bb26ccfc39/640.png",
      "https://image.jiqizhixin.com/uploads/editor/009fa9aa-e54d-4c5c-83a0-47ffa379ec3a/640.png",
      "https://image.jiqizhixin.com/uploads/editor/0007b851-a2c3-49f6-b382-96d63b74f957/640.png",
      "https://image.jiqizhixin.com/uploads/editor/226be799-61cf-4698-9ba1-72c7437654dc/640.png",
      "https://image.jiqizhixin.com/uploads/editor/e2784827-763e-4ff3-8a4d-0ad69308d4c2/640.png",
      "https://image.jiqizhixin.com/uploads/editor/b97a3edf-f80f-414a-9e02-1fce91338871/640.png"
    ]
  },
  {
    "url": "https://www.jiqizhixin.com/articles/2025-01-21-7",
    "title": "首个公开发表的SAR图像目标识别基础模型！国防科大刘永祥&刘丽教授团队提出SARATR-X 1.0",
    "author": "机器之心原创2025/01/21 17:07",
    "date": "2025/01/21 17:07",
    "content": "合成孔径雷达（Synthetic Aperture Radar, SAR）作为一种基于电磁波的主动探测技术，具有全天时、全天候的对地观测能力，已发展成为一种不可或缺的对地观测工具，在军民很多领域均有着重要的应用。\n目标识别（Automatic target recognition，ATR）是 SAR 图像智能解译的核心问题，旨在对 SAR 图像中典型目标（通常为车辆、舰船和飞机等目标）进行自动定位和分类，复杂、开放、对抗环境下的 SAR 目标识别要做到高精准、高敏捷、强稳健、省资源，仍然面临很多挑战。当前，SAR 目标识别主要面临两个层面挑战。\n技术层面\n，SAR 目标识别方法多为有监督、静态、单任务、单模型、单平台，对特定类别的检测和分类，都需要各自的算法模型，每个任务都必须从头开始独立学习，这导致计算冗余、算法设计周期长、泛化能力严重不足、高标注依赖等问题。\n生态层面\n，由于 SAR 图像数据敏感性、标注代价昂贵等因素，缺乏良好的、开源的代码、评估基准和数据生态，导致很多 SAR 目标识别算法不开源、算法评估基准不统一、目前尚无公开的百万 / 千万级大规模高质量 SAR 目标识别基准数据集等问题。\n在人工智能基础模型技术飞速发展的今天，SAR 图像解译领域技术创新与发展生态亟待突破。\n[图片: https://image.jiqizhixin.com/uploads/editor/6b88dd24-eea1-4389-8e0e-4e2bed2c8571/640.png]\n图 1. 各种专门的 SAR ATR 数据集和任务。SAR ATR 包括各种成像条件（即操作条件），如目标、场景和传感器。然而，由于成本较高，通常是在特定任务和设置中收集数据集。例如，MSTAR 是 X 波段和草地场景中的 10 型车辆目标分类数据集，SAR-Aircraft 是从三个机场和 C 波段卫星收集的 7 型飞机检测数据集。不同的目标特征、场景信息和传感器参数使现有算法的泛化困难。因此，团队旨在建立 SAR ATR 基础模型，一种用于各种任务的通用方法。\n为了解决上述技术挑战，国防科技大学电子科学学院刘永祥&刘丽教授团队提出首个公开发表的SAR图像目标识别基础模型SARATR-X 1.0。\n技术层面：\n①率先开展基于自监督学习的 SAR 目标特征表示学习；②创新性地提出了适用于 SAR 图像的联合嵌入 - 预测自监督学习新框架（Joint Embedding Predictive Architecture for SAR ATR, SAR-JEPA），让深度神经网络仅仅预测 SAR 图像稀疏且重要梯度特征表示，有效地抑制了 SAR 图像相干斑噪声，避免预测 SAR 图像含相干斑噪声的原始像素强度信息；③研制了首个 SAR 图像目标识别基础模型 SARATR-X（0.66 亿参数，基于 Transformer），突破了复杂场景中 SAR 目标特征学习对大规模高质量标注数据高度依赖的瓶颈，大幅提升了预训练基础模型的认知能力。\n生态层面：\n团队致力于为 SAR 图像目标识别创建一个良好开源生态，以促进 SAR 目标识别技术快速创新发展。①规范和整合已有公开数据集，形成较大规模 SAR 图像陆海目标识别数据集 SARDet-180K；②为了取代 MSTAR（10 种车辆型号），耗时两年构建 SAR 车辆目标识别数据集 NUDT4MSTAR（40 种车辆型号、更具挑战的实际场景、数据公开、规模超过同类型数据集十倍），进行了详细性能评测；③开源相关的目标识别算法代码和评估基准。\n研究成果以 “SARATR-X：面向 SAR 目标识别的基础模型（SARATR-X: Towards Building A Foundation Model for SAR Target Recognition）” 和 “预测梯度更好：探索联合嵌入-预测框架的 SAR ATR 自监督学习（Predicting gradient is better: Exploring self-supervised learning for SAR ATR with a joint-embedding predictive architecture）”，被国际顶级学术期刊《IEEE Transactions on Image Processing》录用和《ISPRS Journal of Photogrammetry and Remote Sensing》发表。\n团队的代表性工作一经发表、录用后，已经引起国内外同行关注，获得积极评价。引文单位包括美国空军研究实验室、法国古斯塔夫・埃菲尔大学、新加坡南洋理工大学、北京大学、武汉大学、北京航空航天大学等。\n例如，ISPRS Journal 主编、LASTIG 实验室主任 Clement Mallet 在其论文《AnySat: An Earth Observation Model for Any Resolutions, Scales, and Modalities》中认为 “SAR-JEPA [41] 首次将联合嵌入预测框架概念应用于对地观测，专门用于 SAR 数据。（引文原文：SAR-JEPA [41] introduces the first implementation of JEPA concepts for EO, focusing exclusively on SAR data. In this paper, we combine JEPA with a versatile spatial encoder architecture, allowing a single model to handle diverse data scales, resolutions, and modalities.）”\n此外，该团队正在加紧研制 SARATR-X 2.0，预计参数规模 3 亿，SAR 目标切片样本规模 200 万，其中收集的数据将形成开源数据集以服务生态建设，近期将发布 SAR 车辆目标识别数据集 NUDT4MSTAR。\n技术方案\n团队旨在构建一个通用 SAR 图像目标识别基础模型以满足实践中多样的识别任务需求。作为首个公开发布的 SAR 图像目标识别基础模型 SARATR-X 1.0，该模型从大规模无标注 SAR 目标图像中学习到了较为通用的特征表示，突破了传统有监督算法适应性局限，为各种下游任务的高效适应提供基础。在系列工作中，团队研究了 SAR 图像目标识别基础模型的预训练集、模型架构、自监督学习和评估基准。\n预训练集\n，所使用的预训练集包括不同的目标类别和成像条件，以适应各种下游任务，将大部分开源数据集作为预训练的一部分，共纳入了 14 个具有不同目标类别和成像条件的分类和检测数据集，作为新的预训练数据集，以探索基础模型的潜力。\n[图片: https://image.jiqizhixin.com/uploads/editor/11e5610b-7b56-40c8-a449-a8630f2bc3cf/640.png]\n表 1\n.\nSARATR-X 用于预训练的 14 个开源合成孔径雷达数据集。\n模型架构\n，采用 HiViT 架构，旨在实现更好的遥感图像空间表示，特别是对于大图像中的小目标。HiViT 具有 Swin Transformer 高分辨率输入的优势，且可在自监督学习的掩码图像建模中丢弃补丁提高训练效率。\n自监督学习\n，SAR 相干成像中的散斑噪声会对图像质量产生负面影响。此外，SAR 幅度图像的视觉特征不像光学 RGB 图像那样明显。因此，SAR SSL 的主要任务是提高特征学习和目标信号的质量。在前期工作 SAR-JEPA 中，重点研究了如何针对 SAR 图像特性设计自监督学习方法。\nSAR-JEPA 受 JEPA、MaskFeat、FG-MAE 等工作启发，这些工作利用特征空间进行自监督学习任务，而非在原始像素空间进行，这压缩了图像空间中信息冗余，且可以学习到不同特征，如目标性质、深层语义特征。SAR-JEPA 针对 SAR 图像噪声问题，重点在一个降噪特征空间进行自监督学习，通过结合传统特征算子去除散斑噪声干扰，提取目标边缘梯度信息用于自监督，从而实现在 SAR 图像这种噪声数据中的大规模无标注自监督学习。其结果表明自监督学习模型性能可在不同 SAR 目标分类数据集上随着数据量而不断增长。这推动了我们基于大规模数据集构建一个通用 SAR 图像目标识别基础模型，从而实现在不同目标、场景、传感器和识别任务中高效复用。\n因此，SARATR-X 基于 SAR-JEPA 进行训练，首先在 ImageNet 数据进行预训练，以获得更好的初始化模型多样性，第二步是利用 SAR-JEPA 中高质量的目标信号对 SAR 图像进行预训练。\n[图片: https://image.jiqizhixin.com/uploads/editor/ddf36a48-5bf5-4e0d-aa3a-4456c7c064a6/640.png]\n图 2. 两步预训练过程。第一步是对 ImageNet 数据进行预训练，以获得更好的初始化模型多样性。第二步是利用高质量的目标信号对 SAR 图像进行预训练，比如抑制散斑噪声和提取目标边缘的多尺度梯度特征。\n评估任务\n，针对全面评估基础模型的性能需求，团队利用 3 个开源目标数据集，首先构建了一个包含 25 个类别的细粒度分类数据集 SAR-VSA，以评估所提改进措施的有效性。然后，在公开分类和检测数据集上，对所提 SARATR-X 1.0 和现有方法进行了全面比较。\n模型性能\n受限于公开的 SAR 目标识别数据集规模，研制的 SAR 图像目标识别基础模型 SARATR-X 1.0 规模只有 0.66 亿参数，但从大规模无标注 SAR 目标图像中学习到了较为通用的特征表示。在多种下游目标识别任务上（8 个基准目标识别任务，包括小样本目标识别、稳健目标识别、目标检测等）的性能达到国际先进或者领先水平（如下图 3 所示）。在细粒度车辆 MSTAR 数据集中，它的目标分类性能优于现有的 SSL 方法（BIDFC），提升 4.5%。\n此外，它在扩展操作条件 EOCs（擦地角 EOCs-Depression、目标配置 EOCs-Config 和目标版本 EOCs-Version）下表现良好。SARATR-X 在各种类别（多类的 SARDet-100K 和 OGSOD、船舶 SSDD 和飞机 SAR-AIRcraft）的目标检测下也具有竞争力，平均提升约 4%。并且所提方法具有良好的数据量和参数量可扩展性，具有进一步提升潜力。\n[图片: https://image.jiqizhixin.com/uploads/editor/c04f91f3-d6c2-4334-98a1-72c74d8fc052/640.png]\n图 3. SARATR-X 1.0 分类和检测的结果。\n检测结果分析\n，检测可视化如下图 4 所示，虚警和漏检在 SAR 图像中很常见，特别是在相似的目标重叠和复杂的场景。虽然所提方法通过学习图像中的上下文信息，有效地提高了检测效果，但复杂场景和低质量图像的目标检测仍然非常困难。\n[图片: https://image.jiqizhixin.com/uploads/editor/92bbdd0d-b706-447b-bf88-4de5bd7bc8a3/640.png]\n图 4. 在 SARDet-100K 上进行检测的可视化。\n注意力多样性分析\n，对于不同模型的注意力范围进行可视化分析，如图 5 所示，通过模型架构（图 a v.s. 图 b)，初始化权值（图 a v.s. 图 c）和 SSL (图 d v.s. 图 e）改进以确保 SAR 目标识别的注意范围不同，包括 HiViT 架构、ImageNet 权重和 SAR 目标特征。\n[图片: https://image.jiqizhixin.com/uploads/editor/48cac0ce-3cd9-40d5-9569-6abcf8a15137/640.png]\n图 5. 不同注意头的平均注意距离（x 轴为注意头层数，点颜色代表不同的层，以便更好地可视化），注意距离（Attention Distance）代表了一个接受域的范围。\n可扩展性\n，尽管掩码图像建模可以有效地随数据资源和模型参数扩展性能，但在处理噪声数据（如 SAR）时，所提方法是否可以确保其可扩展性？图 6 从三个角度展示了实验的结果：数据集大小、模型参数量和训练轮数。尽管预训练集包含 18 万个图像，比 ImageNet-1K 小，但在图 6（a）和（b）中，随着数据和参数量的增加，下游任务性能呈现显著上升曲线。这一结果表明，通过提取高质量的特征作为引导信号，基础模型可以充分发挥其在 SAR 目标识别中的潜力。但由于数据量限制，模型在扩展训练轮数时倾向于过拟合。此外，SAR 图像噪声和低分辨率进一步加剧了过拟合。\n[图片: https://image.jiqizhixin.com/uploads/editor/80c139b3-fc56-4424-80f6-8bc9aaa4d7ea/640.png]\n图 6. SARATR-X 在数据集大小、模型参数量和训练轮数方面的可扩展性。虽然方法受益于这三个方面，但需要注意的是，由于数据集的大小，过大的训练轮数经常会导致过拟合。\n更多图表分析可见原文。\n论文传送门\nSARATR-X\n题目：SARATR-X: Towards Building A Foundation Model for SAR Target Recognition\n期刊：IEEE Transactions on Image Processing\n论文：https://arxiv.org/abs/2405.09365\n代码：https://github.com/waterdisappear/SARATR-X\n年份：2025\n单位：国防科技大学、上海人工智能实验室\n作者：李玮杰、杨威、侯跃南、刘丽、刘永祥、黎湘\nSAR-JEPA\n题目：Predicting gradient is better: Exploring self-supervised learning for SAR ATR with a joint-embedding predictive architecture\n期刊：ISPRS Journal of Photogrammetry and Remote Sensing\n论文：https://www.sciencedirect.com/science/article/pii/S0924271624003514\n代码：https://github.com/waterdisappear/SAR-JEPA\n年份：2024\n单位：国防科技大学、上海人工智能实验室、南开大学\n作者：李玮杰、杨威、刘天鹏、侯跃南、李宇轩、刘振、刘永祥、刘丽",
    "tags": [
      "入门SARATR-X 1.0国防科技大学目标识别合成孔径雷达",
      "入门",
      "SARATR-X 1.0",
      "国防科技大学",
      "目标识别",
      "合成孔径雷达"
    ],
    "category": "入门",
    "images": [
      "https://image.jiqizhixin.com/uploads/editor/6b88dd24-eea1-4389-8e0e-4e2bed2c8571/640.png",
      "https://image.jiqizhixin.com/uploads/editor/11e5610b-7b56-40c8-a449-a8630f2bc3cf/640.png",
      "https://image.jiqizhixin.com/uploads/editor/ddf36a48-5bf5-4e0d-aa3a-4456c7c064a6/640.png",
      "https://image.jiqizhixin.com/uploads/editor/c04f91f3-d6c2-4334-98a1-72c74d8fc052/640.png",
      "https://image.jiqizhixin.com/uploads/editor/92bbdd0d-b706-447b-bf88-4de5bd7bc8a3/640.png",
      "https://image.jiqizhixin.com/uploads/editor/48cac0ce-3cd9-40d5-9569-6abcf8a15137/640.png",
      "https://image.jiqizhixin.com/uploads/editor/80c139b3-fc56-4424-80f6-8bc9aaa4d7ea/640.png"
    ]
  },
  {
    "url": "https://www.jiqizhixin.com/articles/2025-01-21-6",
    "title": "无直接数据可用，AI怎么学会「干活」？微软团队揭秘AI从语言到行动的进化之路",
    "author": "机器之心原创2025/01/21 17:04",
    "date": "2025/01/21 17:04",
    "content": "[图片: https://image.jiqizhixin.com/uploads/editor/e2af9102-c13b-4d0b-b14d-20f9dea35cd3/640.png]\nAIxiv专栏是机器之心发布学术、技术内容的栏目。过去数年，机器之心AIxiv专栏接收报道了2000多篇内容，覆盖全球各大高校与企业的顶级实验室，有效促进了学术交流与传播。如果您有优秀的工作想要分享，欢迎投稿或者联系报道。投稿邮箱：liyazhou@jiqizhixin.com；zhaoyunfeng@jiqizhixin.com\n该技术报告的主要作者 Lu Wang, Fangkai Yang, Chaoyun  Zhang, Shilin He, Pu Zhao, Si Qin 等均来自 Data, Knowledge, and Intelligence (DKI) 团队，为微软 TaskWeaver, WizardLLM, Windows GUI Agent UFO 的核心开发者。\n近年来，大语言模型（Large Language Models, LLMs）的迅猛发展推动了自然语言处理（NLP）领域的技术进步。这些模型在对话生成、文本翻译、知识问答和代码生成等任务中展现出卓越的性能。\n然而，尽管 LLMs 可以通过语言生成为用户提供信息支持，其功能仍局限于文本层面，无法主动与物理或数字环境交互，或因缺乏领域知识和数据而导致生成的「动作」效果不佳。这种「\n语言 - 行动断层\n」阻碍了人工智能（AI）在许多实际场景中的广泛应用。\n为解决这一核心问题，微软团队首次提出了一种完整的方法体系，详尽描述了在无直接可用数据的情况下如何从零开始训练一个大行动模型（Large Action Model, LAM），并将其逐步构建为可在真实环境中完成任务的智能体。\n这一工作为 LAM 模型训练的奠定了基础，还为 AI 从\n被动语言生成\n向\n主动行动生成\n的转变提供了新思路。\n[图片: https://image.jiqizhixin.com/uploads/editor/db06fe02-96b3-459a-8193-8092d8157d2b/640.png]\n技术报告链接：Large Action Models: From Inception to Implementation\n数据处理代码链接：https://github.com/microsoft/UFO/tree/main/dataflow\n完整的技术文档链接：https://microsoft.github.io/UFO/dataflow/overview/\n从语言到行动的必要演化\nLLMs 的局限性\n传统 LLMs，如 OpenAI 的 GPT 系列和 Mistral-7B，能够生成富有逻辑性和创意的文本内容，广泛应用于问答系统、代码补全、文案生成等任务中。然而，当用户的需求超越语言生成层面，例如操作软件、完成复杂的工作流程或直接操控物理设备时，这些模型便暴露出明显的不足。\n这一局限性源于 LLMs 的设计初衷：它们被优化用于生成语言内容，而非执行行动。虽然 LLMs 在任务规划和意图理解方面表现出色，但它们缺乏行动生成所需的任务分解、环境交互和多步执行能力。\nLAM（大行动模型）具备三大特性：\n用户意图理解\n，能从多种输入（语言、语音、图像等）中准确解析意图并转化为具体可执行计划；\n行动生成能力\n，可根据环境将用户需求转化为 GUI 操作、API 调用、物理动作等多种形式的具体步骤；\n动态规划与适应\n，能够分解复杂任务，灵活应对环境变化，实时调整计划以完成目标。这些特性使 LAM 在复杂任务执行中表现出色。\n[图片: https://image.jiqizhixin.com/uploads/editor/f9eadbfe-de57-4896-98d0-c6f9ae3e21ad/640.png]\n图 1：从 LLM 到 LAM 的演化\n从 LLMs 到 LAMs 的挑战\n如图 1 所示，构建 LAMs 的核心挑战在于如何将模型从一个被动的文本生成器转变为能够在真实环境中执行复杂任务的主动行动生成器。这一转变不仅需要重新定义模型能力，还涉及从数据、训练方法到评估方式的全面革新：\n数据积累的难题\n数据获取是训练 LAM 的最大挑战。LAM 需要大量任务 - 行动对数据来学习如何在不同环境中执行操作。然而，这类数据在实际应用中往往难以获取或批量收集。\n模型训练的重大转化\nLAM 的开发需要从仅生成文本的 LLMs 转化为具备任务规划、动态执行和调整能力的模型。这不仅需要对模型架构进行深度改造，还需要采用全新的训练方法，以赋予模型行动生成与环境适配的能力。\n离线评估的局限性\n在静态、受控环境中测试 LAM 的性能是必要的一步，用以验证其基础能力。然而，仅止步于离线评估无法真实反映模型在实际复杂场景中的表现。\n环境适配与线上评估的复杂性\nLAM 需要实时与复杂、多样的数字或物理环境交互。这要求模型具备动态适应性，能够根据实时反馈调整行动。此外，在真实环境中进行线上评估，测试 LAM 的准确性、效率和任务完成效果，是验证其实际性能的关键环节。\n针对上述挑战，微软团队首次提出并实现了一套完整的从 0 到 1 训练 LAM 模型的流程，涵盖了从数据积累、模型训练到实际部署的所有步骤。\n该团队的方法不仅解决了「无数据」的初始瓶颈，还通过逐步迭代的方式，让模型从简单的任务规划能力成长为具备复杂行动生成能力的智能体。这一研究填补了现有领域的空白，为 LAMs 的开发提供了首个实践范例。\n数据积累\n从无到有构建 LAM 的第一步\n在训练 LAM（大行动模型）时，数据积累是关键。与 LLMs（大语言模型）训练需要大量文本数据类似，LAM 的开发依赖高质量的任务 - 行动数据。\n然而，这类数据在实际应用中非常稀缺，特别是领域专属和可执行的数据。为了克服这一瓶颈，该团队设计了一套从无到有的数据收集与处理流程，分为两大阶段：\n任务 - 计划数据收集\n和\n任务 - 行动数据收集\n。\n[图片: https://image.jiqizhixin.com/uploads/editor/a3f1b89e-16f1-4a6c-ae7a-c5e072659f92/640.png]\n图 2：任务 - 计划数据的收集过程\n阶段一：任务 - 计划数据收集\n如图 2 所示，任务 - 计划数据以用户请求为起点，生成任务描述及其对应的详细操作步骤。该团队从多种开源资源中收集任务 - 计划对，包括应用帮助文档（如 Microsoft Word 的帮助页面）、WikiHow 任务教程，以及用户的搜索查询记录。\n通过这些来源，该团队构建了包含\n76,672\n对任务与计划的初始数据集，其中\n29,182\n对是直接获取的，\n47,490\n对通过数据扩展技术生成。\n此外，他们采用数据增强技术生成更多任务 - 计划对。通过 GPT-4o 演化原始任务，增加复杂性和约束条件，同时生成相应的计划，扩展数据集规模至原来的 150%。例如，「在 Excel 中创建下拉菜单」被演化为「创建依赖下拉菜单，并根据第一列选择过滤第二列内容」，从而提高模型对复杂任务的适应能力。\n[图片: https://image.jiqizhixin.com/uploads/editor/ab2705a5-f3e3-4127-8dc7-c9631f3565b5/640.png]\n图 3：任务 - 行动数据收集过程\n阶段二：任务 - 行动数据收集\n任务 - 计划数据虽然用于高层次规划，但不能直接执行。如图 3 所示，为填补从规划到执行的差距，该团队通过以下步骤生成任务 - 行动数据：\n1. 实例化任务：利用预定义模板（如 Word 文档样例），将任务描述具体化，将抽象的计划步骤转化为具体的行动序列（如「点击菜单栏中的「设计」选项」）。\n2. 执行验证：在真实环境中执行实例化的任务，捕获执行轨迹和环境反馈，确保行动序列的可操作性和正确性。\n3. 评估与后处理：使用 GPT-4o 对执行结果进行验证，仅保留与任务目标一致的成功轨迹，并记录详细元数据（如环境状态和执行时间），最终生成结构化的任务 - 行动对。\n这一流程最终生成了覆盖广泛操作场景的任务 - 行动数据集，为 LAM 训练提供了精确的行动模板，显著提升了模型在真实环境中的任务执行能力。\n通过两阶段的逐步积累，成功地从「无数据」状态出发，构建了 LAM 训练所需的高质量任务 - 行动数据。这一方法不仅解决了数据稀缺问题，还通过引入真实环境交互和动态验证，确保数据的高效性和适用性，为从 LLMs 到 LAMs 的转变提供了坚实基础。\n方法：从 0 到 1，逐步构建 LAM\n如图 4 所示，构建 LAM 的过程分为四个阶段，涵盖了从数据积累到模型训练的完整工作流。\n[图片: https://image.jiqizhixin.com/uploads/editor/2f07e1fb-e822-4803-94ab-3e9941dcbd43/640.png]\n图 4：LAM 的训练过程\n第一阶段：任务计划预训练\n为了让模型具备基本的任务规划能力，首先训练模型生成任务分解计划。数据来源为任务 - 计划数据。模型的目标是根据输入任务生成正确的任务分解计划。例如，「在 Word 中插入表格」被分解为「点击插入菜单」、「选择表格选项」、「输入表格行列数」等步骤。这一阶段让模型掌握了任务分解的基本能力，为后续的行动生成打下了基础。\n第二阶段：专家知识学习\n尽管第一阶段的模型可以生成任务计划，但仍缺乏执行这些计划的能力。为此，需要利用收集到的任务 - 行动数据，并通过模仿学习训练模型执行具体操作。经过训练，模型从一个被动的计划生成器转变为能够执行计划的主动行动生成器。\n第三阶段：自我探索提升\n专家数据的覆盖范围有限，无法囊括所有可能的任务场景。为此，该团队设计了自我探索机制，将 LAM 部署在 UFO 中，UFO 是一个开源 GUI Agent 框架，能够通过交互 Windows 操作系统中的图形用户界面（GUI）元素来完成任务。让 LAM 尝试完成之前失败的任务，并从中积累新的成功经验。\n1. 任务挑战：模型尝试完成 2,284 个由 GPT-4 未解决的任务，通过动态探索生成可能的成功轨迹。\n2. 数据扩展：在自我探索中，模型生成了 496 条新成功轨迹，将其与之前的专家数据合并形成扩展数据集。\n3. 模型迭代：通过再次微调，模型进一步提升了处理复杂任务的能力，增强了对未知环境的适应性。\n这一阶段实现了从无数据到新数据的自动生成与积累，扩展了训练数据的覆盖范围。\n第四阶段：奖励模型优化\n为了进一步提升模型的行动质量，在此引入了奖励模型（Reward Model, RM），同时利用正负反馈，通过强化学习优化 LAM 的决策能力。\n实验结果\n离线实验结果\n[图片: https://image.jiqizhixin.com/uploads/editor/5a583540-f4d3-49aa-8571-9514259b2999/640.png]\n表格 1：不同 LAM 训练阶段的离线实验结果\n为了验证训练方法的有效性，该团队在 435 个任务上对不同阶段的 LAM 模型进行了离线测试。如表格 1 的实验结果显示，LAM 的各阶段的训练都带来了模型性能提升。\n环境适配\n[图片: https://image.jiqizhixin.com/uploads/editor/a8384a93-fa0d-4422-9e7e-8de1461fc10b/640.png]\n图 5：LAM 智能体架构\n如图 5 所示，经过训练的 LAM 模型被集成到 GUI 智能体 UFO 的 AppAgent 中作为推理引擎，后者充当桥梁，将 LAM 预测的动作「着地」为可执行的实际操作。\n线上实验结果\n[图片: https://image.jiqizhixin.com/uploads/editor/0b8c3e1b-6055-4401-87ef-97faf3c5e6a4/640.png]\n表格 2：LAM 的线上实验结果\n如表格 2 所示，LAM 在线上实验任务中成功率（TSR）方面表现优异，达到\n71.0%\n，在文本输入模式下超越了基线模型（GPT-4o 和 GPT-4o Mini）。\n效率对比\nLAM 在任务完成时间和平均步时延上展现了显著优势：\n1. 任务完成时间：LAM 完成单个任务平均耗时仅\n30.42\n秒，相比之下，无视觉输入的 GPT-4o 耗时 86.42 秒，约为 LAM 的 2.84 倍，而带视觉输入的 GPT-4o 耗时更长，为 96.48 秒。\n2. 平均步时延：LAM 的每步时延为\n5.41\n秒，显著优于无视觉输入的 GPT-4o（12.84 秒）和带视觉输入的 GPT-4o（19.36 秒）。\n更多细节，请参阅技术报告原文。",
    "tags": [
      "工程LAM微软",
      "工程",
      "LAM",
      "微软"
    ],
    "category": "工程",
    "images": [
      "https://image.jiqizhixin.com/uploads/editor/e2af9102-c13b-4d0b-b14d-20f9dea35cd3/640.png",
      "https://image.jiqizhixin.com/uploads/editor/db06fe02-96b3-459a-8193-8092d8157d2b/640.png",
      "https://image.jiqizhixin.com/uploads/editor/f9eadbfe-de57-4896-98d0-c6f9ae3e21ad/640.png",
      "https://image.jiqizhixin.com/uploads/editor/a3f1b89e-16f1-4a6c-ae7a-c5e072659f92/640.png",
      "https://image.jiqizhixin.com/uploads/editor/ab2705a5-f3e3-4127-8dc7-c9631f3565b5/640.png",
      "https://image.jiqizhixin.com/uploads/editor/2f07e1fb-e822-4803-94ab-3e9941dcbd43/640.png",
      "https://image.jiqizhixin.com/uploads/editor/5a583540-f4d3-49aa-8571-9514259b2999/640.png",
      "https://image.jiqizhixin.com/uploads/editor/a8384a93-fa0d-4422-9e7e-8de1461fc10b/640.png",
      "https://image.jiqizhixin.com/uploads/editor/0b8c3e1b-6055-4401-87ef-97faf3c5e6a4/640.png"
    ]
  },
  {
    "url": "https://www.jiqizhixin.com/articles/2025-01-21-5",
    "title": "「DeepSeek接班OpenAI」，最新开源的R1推理模型，让AI圈爆了",
    "author": "机器之心原创2025/01/21 12:43",
    "date": "2025/01/21 12:43",
    "content": "OpenAI 的最初愿景，最终被一家国内创业公司实现了？\n昨晚，大模型领域再次「热闹起来」，月之暗面发布在数学、代码、多模态推理能力层面全面对标 OpenAI 的满血版 o1 的\n多模态思考模型 K1.5\n。而最近大热的 DeepSeek 正式推出了 DeepSeek-R1，同样在数学、代码和自然语言推理等任务上比肩 OpenAI o1 正式版。\n去年 12 月开源的大模型 DeepSeek-V3 刚刚掀起了一阵热潮，实现了诸多的不可能。这次开源的 R1 大模型则在一开始就让一众 AI 研究者感到「震惊」，人们纷纷在猜测这是如何做到的。\n[图片: https://image.jiqizhixin.com/uploads/editor/bd72b972-20c1-4636-8a13-3e64fd6955ed/640.png]\nAutoAWQ 作者 Casper Hansen 表示，DeepSeek-R1 使用一种多阶段循环的训练方式：基础→ RL →微调→ RL →微调→ RL。\nUC Berkeley 教授 Alex Dimakis 则认为\nDeepSeek 现在已经处于领先位置，美国公司可能需要迎头赶上了\n。\n[图片: https://image.jiqizhixin.com/uploads/editor/fee05793-ab01-4239-92b9-1ddc2272c5e7/640.png]\n目前，DeepSeek 在网页端、App 端和 API 端全面上线了 R1，下图为网页端对话界面，选择 DeepSeek-R1 就能直接体验。\n[图片: https://image.jiqizhixin.com/uploads/editor/d897c0da-bfc1-4213-904d-b4bdf07117f3/640.png]\n体验地址：https://www.deepseek.com/\n此次，DeepSeek 发布了两个参数为 660B 的 DeepSeek-R1-Zero 和 DeepSeek-R1，并选择开源了模型权重，同时允许用户使用 R1 来训练其他模型。\n在技术层面，R1 在后训练阶段大规模使用了强化学习（RL）技术，在仅用非常少标注数据的情况下，极大提升了模型推理能力。下图为 R1 与 o1-1217、o1-mini、自家 DeepSeek-V3 在多个数据集上的性能比较，可以看到，R1 与 o1-1217 不相上下、互有胜负。\n[图片: https://image.jiqizhixin.com/uploads/editor/79c916cc-6495-46ad-ab8c-dc6acad827b0/640.png]\n另外，DeepSeek-R1 蒸馏出了六个小模型，参数从小到大分别为 1.5B、7B、8B、14B、32B 以及 70B。这六个模型同样完全开源，旨在回馈开源社区，推动「Open AI」的边界。\n[图片: https://image.jiqizhixin.com/uploads/editor/a8830e43-19e0-4bf8-9f44-eeeff4490c3c/640.png]\n模型下载地址：https://huggingface.co/deepseek-ai?continueFlag=f18057c998f54575cb0608a591c993fb\n性能方面，蒸馏后的 R1 32B 和 70B 版本远远超过了 GPT-4o、Claude 3.5 Sonnet 和 QwQ-32B，并逼近 o1-mini。\n[图片: https://image.jiqizhixin.com/uploads/editor/7a1f04b2-fdb1-4154-a900-99b597377e5e/640.png]\n至于很多开发者关心的 DeepSeek-R1 API 价格，可以说是一如既往地给力。\nDeepSeek-R1 API 服务的定价为每百万输入 tokens 1 元（缓存命中）/ 4 元（缓存未命中），每百万输出 tokens 16 元。\n[图片: https://image.jiqizhixin.com/uploads/editor/0dfd9f94-f01f-42fc-bec9-c814d162ee2f/640.png]\n显然，与 o1 的 API 定价比起来（每百万输入 tokens 15 美元、每百万输出 tokens 60 美元），DeepSeek 具有极高的性价比。\n[图片: https://image.jiqizhixin.com/uploads/editor/2bd4ec97-dc5f-476c-bd1b-39eeb822b53c/640.png]\nDeepSeek 秉持了开源到底的决心，将 R1 模型的训练技术全部开放，放出了背后的研究论文。\n[图片: https://image.jiqizhixin.com/uploads/editor/78d6a7ee-bee9-4b30-9652-978a4359bd4a/640.png]\n论文链接：\nhttps://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf\nR1 技术报告\n以往的研究主要依赖大量的监督数据来提升模型性能。DeepSeek 的开发团队则开辟了一种全新的思路：\n即使不用监督微调（SFT）作为冷启动，通过大规模强化学习也能显著提升模型的推理能力\n。如果再加上少量的冷启动数据，效果会更好。\n为了做到这一点，他们开发了 DeepSeek-R1-Zero。具体来说，DeepSeek-R1-Zero 主要有以下三点独特的设计：\n首先是采用了\n群组相对策略优化（GRPO）\n来降低训练成本。GRPO 不需要使用与策略模型同样大小的评估模型，而是直接从群组分数中估算基线。\n对于每个输入问题 q，GRPO 算法会从旧策略中采样一组输出 {o1, o2, ..., oG}，形成评估群组，然后通过最大化目标函数来优化策略模型：\n[图片: https://image.jiqizhixin.com/uploads/editor/fe623a30-3ffb-4b0f-a919-ed117ab95774/640.png]\n其中，优势值 A_i 通过标准化每个输出的奖励来计算：\n[图片: https://image.jiqizhixin.com/uploads/editor/8a83869c-66fd-40b7-bc90-198d3525786b/640.png]\n其次是奖励设计。如何设计奖励，决定着 RL 优化的方向。DeepSeek 给出的解法是采用\n准确度和格式\n两种互补的奖励机制。\n准确度奖励用于评估回答的正确性。在数学题中，模型需要用特定格式给出答案以便验证；在编程题中，则通过编译器运行测试用例获取反馈。\n第二种是\n格式奖励\n，模型需要将思考过程放在 '<think>' 和 '</think>' 这两个特定的标签之间，提升输出的规范性。\n该团队没有使用常用的神经网络奖励模型，是因为在大规模强化学习过程中，模型可能会出现「作弊」问题。同时也避免了重新训练奖励模型需要额外资源，简化了训练流程。\n第三点是\n训练模版\n，在 GRPO 和奖励设计的基础上，开发团队设计了如表 1 所示的简单模板来引导基础模型。这个模板要求 DeepSeek-R1-Zero 先给出推理过程，再提供最终答案。这种设计仅规范了基本结构，不对内容施加任何限制或偏见，比如不强制要求使用反思性推理或特定解题方法。这种最小干预的设计能够清晰地观察模型在 RL 的进步过程。\n[图片: https://image.jiqizhixin.com/uploads/editor/d96a4990-9503-4df8-9c78-ec3816df3c6f/640.png]\nDeepSeek-R1-Zero 的提升也非常显著。如图 2 所示，做 2024 年的 AIME 数学奥赛试卷，DeepSeek-R1-Zero 的平均 pass@1 分数从最初的 15.6% 显著提升到了 71.0%，达到了与 OpenAI-o1-0912 相当的水平。在多数投票机制中，DeepSeek-R1-Zero 在 AIME 中的成功率进一步提升到了 86.7%，甚至超过了 OpenAI-o1-0912 的表现。\n[图片: https://image.jiqizhixin.com/uploads/editor/a4dc6538-b645-44d7-a487-e38e254d623f/640.png]\n[图片: https://image.jiqizhixin.com/uploads/editor/a54e6dd9-f7e7-44c3-aa96-395b481cfc50/640.png]\nDeepSeek-R1-Zero 与 OpenAI 的 o1-0912 在多个推理相关基准测试上的得分对比。\n在训练过程中，DeepSeek-R1-Zero 展现出了显著的自我进化能力。它学会了生成数百到数千个推理 token，能够更深入地探索和完善思维过程。\n随着训练的深入，模型也发展出了一些高级行为，比如反思能力和探索不同解题方法的能力。这些都不是预先设定的，而是模型在强化学习环境中自然产生的。\n特别值得一提的是，开发团队观察到了一个有趣的「Aha Moment」。在训练的中期阶段，DeepSeek-R1-Zero 学会了通过重新评估初始方法来更合理地分配思考时间。这可能就是强化学习的魅力：只要提供正确的奖励机制，模型就能自主发展出高级的解题策略。\n不过 DeepSeek-R1-Zero 仍然存在一些局限性，如回答的可读性差、语言混杂等问题。\n利用冷启动进行强化学习\n与 DeepSeek-R1-Zero 不同，为了防止基础模型在 RL 训练早期出现不稳定的冷启动阶段，开发团队针对 R1 构建并收集了少量的长 CoT 数据，以作为初始 RL actor 对模型进行微调。为了收集此类数据，开发团队探索了几种方法：以长 CoT 的少样本提示为例、直接提示模型通过反思和验证生成详细答案、以可读格式收集 DeepSeek-R1-Zero 输出、以及通过人工注释者的后处理来细化结果。\nDeepSeek 收集了数千个冷启动数据，以微调 DeepSeek-V3-Base 作为 RL 的起点。与 DeepSeek-R1-Zero 相比，冷启动数据的优势包括：\n可读性\n：DeepSeek-R1-Zero 的一个主要限制是其内容通常不适合阅读。响应可能混合多种语言或缺乏 markdown 格式来为用户突出显示答案。相比之下，在为 R1 创建冷启动数据时，开发团队设计了一个可读模式，在每个响应末尾包含一个摘要，并过滤掉不友好的响应。\n潜力\n：通过精心设计具有人类先验知识的冷启动数据模式，开发团队观察到相较于 DeepSeek-R1-Zero 更好的性能。开发团队相信迭代训练是推理模型的更好方法。\n推理导向的强化学习\n在利用冷启动数据上对 DeepSeek-V3-Base 进行微调后，开发团队采用与 DeepSeek-R1-Zero 相同的大规模强化学习训练流程。此阶段侧重于增强模型的推理能力，特别是在编码、数学、科学和逻辑推理等推理密集型任务中。\n为了缓解语言混合的问题，开发团队在 RL 训练中引入了语言一致性奖励，其计算方式为 CoT 中目标语言单词的比例。虽然消融实验表明这种对齐会导致模型性能略有下降，但这种奖励符合人类偏好，更具可读性。\n最后，开发团队将推理任务的准确率和语言一致性的奖励直接相加，形成最终奖励。然后对微调后的模型进行强化学习 (RL) 训练，直到它在推理任务上实现收敛。\n拒绝采样和监督微调\n当面向推理导向的强化学习收敛时，开发团队利用生成的检查点为后续轮次收集 SFT（监督微调）数据。此阶段结合了来自其他领域的数据，以增强模型在写作、角色扮演和其他通用任务中的能力。\n开发团队通过从上述强化学习训练的检查点执行拒绝采样来整理推理提示并生成推理轨迹。此阶段通过合并其他数据扩展数据集，其中一些数据使用生成奖励模型，将基本事实和模型预测输入 DeepSeek-V3 进行判断。\n此外，开发团队过滤掉了混合语言、长段落和代码块的思路链。对于每个提示，他们会抽取多个答案，并仅保留正确的答案。最终，开发团队收集了约 60 万个推理相关的训练样本。\n用于所有场景的强化学习\n为了进一步使模型与人类偏好保持一致，这里还要实施第二阶段强化学习，旨在提高模型的有用性和无害性，同时完善其推理能力。\n具体来说，研究人员使用奖励信号和各种提示分布的组合来训练模型。对于推理数据，遵循 DeepSeek-R1-Zero 中概述的方法，该方法利用基于规则的奖励来指导数学、代码和逻辑推理领域的学习过程；对于一般数据，则采用奖励模型来捕捉复杂而微妙的场景中的人类偏好。\n最终，奖励信号和多样化数据分布的整合使我们能够训练出一个在推理方面表现出色的模型，同时优先考虑有用性和无害性。\n蒸馏：让小模型具备推理能力\n为了使更高效的小模型具备 DeekSeek-R1 那样的推理能力，开发团队还直接使用 DeepSeek-R1 整理的 80 万个样本对 Qwen 和 Llama 等开源模型进行了微调。研究结果表明，这种简单的蒸馏方法显著增强了小模型的推理能力。\n得益于以上多项技术的创新，开发团队的大量基准测试表明，DeepSeek-R1 实现了比肩业内 SOTA 推理大模型的硬实力，具体可以参考以下结果：\n[图片: https://image.jiqizhixin.com/uploads/editor/16d1573d-9285-47ac-84bb-b4489edd4150/640.png]\n[图片: https://image.jiqizhixin.com/uploads/editor/e205c658-e914-4cb2-a421-c81888d1f284/640.png]\n更多技术细节请参阅原论文。",
    "tags": [
      "产业DeepSeek-R1DeepSeek",
      "产业",
      "DeepSeek-R1",
      "DeepSeek"
    ],
    "category": "产业",
    "images": [
      "https://image.jiqizhixin.com/uploads/editor/bd72b972-20c1-4636-8a13-3e64fd6955ed/640.png",
      "https://image.jiqizhixin.com/uploads/editor/fee05793-ab01-4239-92b9-1ddc2272c5e7/640.png",
      "https://image.jiqizhixin.com/uploads/editor/d897c0da-bfc1-4213-904d-b4bdf07117f3/640.png",
      "https://image.jiqizhixin.com/uploads/editor/79c916cc-6495-46ad-ab8c-dc6acad827b0/640.png",
      "https://image.jiqizhixin.com/uploads/editor/a8830e43-19e0-4bf8-9f44-eeeff4490c3c/640.png",
      "https://image.jiqizhixin.com/uploads/editor/7a1f04b2-fdb1-4154-a900-99b597377e5e/640.png",
      "https://image.jiqizhixin.com/uploads/editor/0dfd9f94-f01f-42fc-bec9-c814d162ee2f/640.png",
      "https://image.jiqizhixin.com/uploads/editor/2bd4ec97-dc5f-476c-bd1b-39eeb822b53c/640.png",
      "https://image.jiqizhixin.com/uploads/editor/78d6a7ee-bee9-4b30-9652-978a4359bd4a/640.png",
      "https://image.jiqizhixin.com/uploads/editor/fe623a30-3ffb-4b0f-a919-ed117ab95774/640.png",
      "https://image.jiqizhixin.com/uploads/editor/8a83869c-66fd-40b7-bc90-198d3525786b/640.png",
      "https://image.jiqizhixin.com/uploads/editor/d96a4990-9503-4df8-9c78-ec3816df3c6f/640.png",
      "https://image.jiqizhixin.com/uploads/editor/a4dc6538-b645-44d7-a487-e38e254d623f/640.png",
      "https://image.jiqizhixin.com/uploads/editor/a54e6dd9-f7e7-44c3-aa96-395b481cfc50/640.png",
      "https://image.jiqizhixin.com/uploads/editor/16d1573d-9285-47ac-84bb-b4489edd4150/640.png",
      "https://image.jiqizhixin.com/uploads/editor/e205c658-e914-4cb2-a421-c81888d1f284/640.png"
    ]
  },
  {
    "url": "https://www.jiqizhixin.com/articles/2025-01-21-4",
    "title": "追平满血版o1的国产多模态模型终于来了！训练细节全部公开",
    "author": "机器之心原创2025/01/21 12:40",
    "date": "2025/01/21 12:40",
    "content": "春节前最后一周，能媲美 Open AI 满血版 o1（Full Version，而非 preview）的模型终于出现了！\n刚刚，月之暗面公布了他们的 Kimi k 系列模型最新版本 ——\nk1.5 多模态思考模型\n。新模型在数学、代码、多模态推理能力等方面全面对标 Open AI 满血版 o1，而且是 OpenAI 之外首个多模态 o1。尤其是 kimi-k1.5-short，成为 SOTA short cot 模型，并大幅领先 GPT-4o 和 Claude 3.5 Sonnet（提升幅度高达 550%）\n[图片: https://image.jiqizhixin.com/uploads/editor/e13f4776-f1d4-4a39-b74c-bfbe9fa31caf/640.png]\n这是 Open AI 之外，首次有模型在数学和代码能力上达到满血 o1，月之暗面也是国内第一个达到该水平的 AI 公司\n。在此之前，部分模型在各类 Benchmark 上可以达到 50 分、60 分的水平（相当于 o1-preview），而 o1 满血版是 80 分、90 分水平，Kimi k1.5 的成绩令人眼前一亮。\n这一切是怎么做到的呢？在 Kimi 技术团队同步发布的技术报告中，我们可以看到他们在新技术范式下的模型训练技术探索之路。\n[图片: https://image.jiqizhixin.com/uploads/editor/67549233-f3ee-475f-b246-03e97610e8d9/640.png]\n技术报告：Kimi k1.5：借助大语言模型实现强化学习的 Scaling\n报告链接：https://github.com/MoonshotAI/kimi-k1.5\n这种技术透明度在当前竞争激烈的大模型市场上并不多见。在谈及为什么要这么做时，月之暗面表示，「因为我们意识到，AGI 之旅才刚刚开始。我们想让更多技术人才了解我们在做的事情，加入我们一起做到更多」。\nKimi k1.5 多项测试，全部 SOTA\n从技术报告来看，Kimi k1.5 多模态推理模型实现了 SOTA （state-of-the-art）级别的推理和通用能力，具体而言：\n在 long-CoT 模式下，Kimi k1.5 在数学、代码及多模态推理能力上，达到长思考 SOTA 模型 OpenAI o1 正式版的水平。Kimi k1.5 在 AIME 上达到 77.5 分，在 MATH 500 上达到 96.2 分，在 Codeforces 上达到 94 百分位，在 MathVista 上达到 74.9 分。\n这应该是全球范围内，OpenAI 之外的公司首次实现 o1 满血版性能。此前的模型只能达到 o1-preview 或 o1-mini 的推理能力。\n[图片: https://image.jiqizhixin.com/uploads/editor/8cdc4642-59b4-4347-93cb-f80de7956e96/640.png]\n在 short-CoT 模式下，Kimi k1.5 在数学、代码、视觉多模态和通用能力上，也达到了全球范围内短思考 SOTA 模型 ，并大幅领先 GPT-4o 和 Claude 3.5 Sonnet 的水平。比如，Kimi k1.5 在 AIME 上达到 60.8 分，MATH500 上达到 94.6 分，LiveCodeBench 上达到 47.3 分。\n[图片: https://image.jiqizhixin.com/uploads/editor/c8307307-7fbf-43a0-88f0-ebbe043a5817/640.png]\n不仅如此，从全球前沿大模型数学竞赛和编程竞赛基准测试来看，Kimi k1.5 的表现也相当不错，处于全球第一梯队，而这两项测试代表了人类智商巅峰。\n[图片: https://image.jiqizhixin.com/uploads/editor/301c8b21-0a16-43ff-80a8-10f8a038e0d8/640.png]\n总之，从 Benchmark 数据来看，k1.5 的推理能力实现了很大提升，可以帮助我们解锁更难的代码、数学、生活等问题。\nKimi k1.5 是怎么练成的？\n随着模型尺寸逐渐增大，预训练阶段参数 scaling up 带来的边际收益开始递减，如果想要深度提升模型推理能力和长程问题能力，基于强化学习的 Post-Training 将会成为下一个突破点 [1]，因为 scaling 强化学习为人工智能的持续进步开辟了新的维度，它使得大语言模型能够通过带有奖励的探索学习来扩展其训练数据，从而也实现计算规模的扩展。\n大的方向非常明确，然而，此前发表的研究工作尚未产生具有竞争力的结果。\n有鉴于此，Kimi 技术团队在 Kimi k1.5 的训练实践中全面探索了 RL 训练技术、多模态数据配方和基础设施优化。\n难得的是，他们探索出的 RL 框架简单、有效，无需依赖蒙特卡洛树搜索、价值函数和过程奖励模型等更复杂的技术也能取得优异的性能。\n此外，他们还提出了有效的 long2short 技术，利用 Long-CoT 技术来改进 Short-CoT 模型，使得模型在短链思维推理方面取得了最佳成果。\n简单、有效的 RL 框架\nKimi 技术团队设计的简单而有效的 RL 框架离不开两个关键要素：\n长上下文 scaling 和改进的策略优化\n。\n先说长上下文 scaling。他们将强化学习的上下文窗口 scale 到 128k，并观察到随着上下文长度的增加，模型性能持续改善。新方法背后的一个关键理念是使用 partial rollout 来提高训练效率 —— 即通过重用大量以前的轨迹来采样新的轨迹，避免从头重新生成新轨迹的成本。技术团队的观察表明，上下文长度是大语言模型强化学习持续 scaling 的一个关键维度。\n再来看策略优化的改进。他们推导出了一个具有 long-CoT 的强化学习公式，并采用在线镜像下降法的变体来实现稳健的策略优化。通过有效的采样策略、长度惩罚和数据配方的优化，他们进一步改进了该算法。\n[图片: https://image.jiqizhixin.com/uploads/editor/d3958dc5-d074-4e27-ac77-f72438f20e00/640.png]\n通过将这两个关键要素结合，Kimi 技术团队建立了一个用于 LLM 学习的简化强化学习框架。由于该框架能够 scale 上下文长度，学习到的 CoT 展现出规划、反思和纠正的特性。增加的上下文长度具有增加搜索步骤数量的效果。因此，他们表明无需依赖蒙特卡洛树搜索、价值函数和过程奖励模型等更复杂的技术也能实现强大的性能。\n此外，他们的模型还在文本和视觉数据上进行了联合训练，具备对这两种模态进行联合推理的能力。\nlong2short 技术\n尽管 long-CoT 模型在性能上表现出色，但与标准的 short-CoT LLM 相比，它在测试时消耗的 token 数量更多。然而，Kimi 技术团队发现将 long-CoT 模型的思维先验迁移到 short-CoT 模型中是可能的，从而在有限的测试 token 预算下提升性能。\n他们提出了几种解决这一 long2short 问题的方法，包括模型融合、最短拒绝采样、DPO 以及 long2short RL。以下是这些方法的详细描述：\n模型融合。团队人员发现模型融合（Model Merging）有助于保持模型的泛化能力。他们还发现，在融合 long-CoT 模型和 short-CoT 模型时，模型融合也能有效提升 token 效率。这种方法通过将 long-CoT 模型与 short-CoT 模型结合，从而在不进行训练的情况下获得一个新模型。具体来说，他们通过简单地平均两个模型的权重来实现融合。\n最短拒绝采样。研究者观察到，模型在回答相同问题时生成的响应长度存在较大差异。基于此，他们设计了最短拒绝采样（Shortest Rejection Sampling）方法。该方法对同一个问题采样 n 次（实验中，n=8），并选择最短的正确响应进行监督微调。\nDPO。与最短拒绝采样类似，团队人员利用 Long CoT 模型生成多个响应样本。并选择最短的正确解决方案作为正样本，而较长的响应则被视为负样本，包括错误的较长响应和正确的较长响应。这些正负样本对构成了用于 DPO 训练的成对偏好数据。\nLong2short RL。在标准的 RL 训练阶段之后，团队人员选择一个在性能和 token 效率之间达到最佳平衡的模型作为基础模型，并进行单独的 long2short RL 训练阶段。在这个第二阶段中，他们还应用了长度惩罚机制，从而显著减少最大 rollout 长度，以进一步惩罚那些超出期望长度但可能正确的响应。\n除了以上这些，Kimi k1.5 的技术报告还透露了很多信息。感兴趣的读者可以去阅读原文。\n2025：加速升级 k 系列强化学习模型\nOpenAI 于 2024 年 5 月、9 月推出的 GPT-4o、o1 两个模型，分别代表了多模态理解、强化学习两条技术路线。在这两条路线上，国内 AI 公司都在陆续发力，并在最近展开了激烈竞争。如今，Kimi 模型在能力上最接近 o1，这让外界对这家公司在 2025 年的表现充满了期待。\n月之暗面表示，2025 年，他们会继续加速升级 k 系列强化学习模型，带来更多模态、更多领域的能力和更强的通用能力。\n[图片: https://image.jiqizhixin.com/uploads/editor/60ed840a-3fac-41b9-b990-d744629fd5b1/640.png]\n我们也期待新模型的早日上线！\n参考链接：[1] https://mp.weixin.qq.com/s/FXGdJA8OyZvLl89rXJiyAQ",
    "tags": [
      "产业Kimi月之暗面",
      "产业",
      "Kimi",
      "月之暗面"
    ],
    "category": "产业",
    "images": [
      "https://image.jiqizhixin.com/uploads/editor/e13f4776-f1d4-4a39-b74c-bfbe9fa31caf/640.png",
      "https://image.jiqizhixin.com/uploads/editor/67549233-f3ee-475f-b246-03e97610e8d9/640.png",
      "https://image.jiqizhixin.com/uploads/editor/8cdc4642-59b4-4347-93cb-f80de7956e96/640.png",
      "https://image.jiqizhixin.com/uploads/editor/c8307307-7fbf-43a0-88f0-ebbe043a5817/640.png",
      "https://image.jiqizhixin.com/uploads/editor/301c8b21-0a16-43ff-80a8-10f8a038e0d8/640.png",
      "https://image.jiqizhixin.com/uploads/editor/d3958dc5-d074-4e27-ac77-f72438f20e00/640.png",
      "https://image.jiqizhixin.com/uploads/editor/60ed840a-3fac-41b9-b990-d744629fd5b1/640.png"
    ]
  },
  {
    "url": "https://www.jiqizhixin.com/articles/2025-01-21-12",
    "title": "AI病毒进化预测新突破，北大团队进化启发通用预测框架登Nature子刊",
    "author": "ScienceAI原创2025/01/21 11:14",
    "date": "2025/01/21 11:14",
    "content": "[图片: https://image.jiqizhixin.com/uploads/editor/ef42d330-06d5-4d54-894f-82c2458dd3e1/640.png]\n编辑 ｜ScienceAI\n在自然界，物种多样性与生物体内承载功能的蛋白质相互约束，这是因为蛋白质作为功能的载体决定了生物的性状，而这些性状经过选择压力筛选后形成了当下的物种多样性分布。从达尔文进化论角度来看，所有的进化都是基因适应环境的效应。\n受此启发，北京大学信息工程学院田永鸿教授、陈杰副教授指导博士生聂志伟、硕士生刘旭东基于进化论视角重新审视病毒进化预测难题，提出了解决病毒进化两大本质问题的跨病毒类型、跨毒株类型的通用进化预测模型，为疫苗、药物的快速主动更新以及提高人类对于新发病毒感染的响应速度提供了强大工具，支撑和加速对于物种复杂进化机制的探索。\n该研究以「\nA unified evolution-driven deep learning framework for virus variation driver prediction\n」为题于2025年1月17日正式发表在《\nNature Machine Intelligence\n》上。\n[图片: https://image.jiqizhixin.com/uploads/editor/001269ea-e7d6-44eb-b9e3-6afc9f30015a/640.png]\n论文链接：\nhttps://www.nature.com/articles/s42256-024-00966-9\n研究亮点‍\n（1）探讨了如何定制化蛋白质语言模型以适配进化预测任务，提出了定制化预训练策略和数据集，为蛋白质语言模型预训练与下游任务之间的权衡提供了研究新视角；\n（2）从进化论角度凝练了病毒进化的两大本质问题，从而通过「微弱突变放大」和「稀少有益突变挖掘」两个创新设计实现了跨病毒类型和跨毒株类型的通用预测，实现了 Science 和 AI 架构的高度融合；\n（3）突变所处相互作用网络的全面重建模块（包含动态粒度注意力机制以挖掘 motif 模式）以及提出的多任务焦点损失函数适用于蛋白质通用体系，可进一步拓展用于各类蛋白质性质预测及蛋白质定向进化；\n（4）实现了不同尺度的病毒进化预测，未来可与疫苗和蛋白类药物设计流程相结合，有望显著提升设计效率和设计可控度。\n进化启发的通用预测框架\n突变是病毒进化的基石，不同病毒的具体进化历程各有其独特性，但是其共性在于最终的进化结果中几乎都是有害突变占据大多数。\n从整个进展尺度来看，即使有害突变与有益突变的比例会随物种和环境不同而有所区别，但是有害突变被认为总是远多于有益突变，即有益突变是病毒蛋白进化适应度空间中的极小子集。\n很自然地，有害突变的高发性使得同一个变异株内难以共存较多的突变，即一个变异株所具有的突变数量与原始型相比往往较少，仅有少数位点会发生突变。\n因此，研究团队将上述病毒进化轨迹凝练为病毒进化的两大本质特点：「少数位点突变」（Few-site mutations）和「稀少有益突变」（Rare beneficial mutations）。\n这两大进化特点导致了明显的建模难题，「少数位点突变」引起的分子内相互作用网络的变化相对比较微弱，这使得神经网络直接捕获是极其困难的，而「稀少有益突变」在数据层面造成了极其严重的正负样本不平衡问题，这对于精准预测对于病毒生存至关重要的稀少有益突变造成了巨大挑战。\n[图片: https://image.jiqizhixin.com/uploads/editor/643ec236-726a-482f-bfdb-67bd0ef8f8f3/640.png]\n图 1：E2VD 模型架构。（来源：论文）\n为此，研究团队提出了进化驱动的病毒变异驱动力预测框架 E2VD（图 1），通过「微弱突变放大」和「稀少有益突变挖掘」两个创新设计实现了跨病毒类型和跨毒株类型的统一预测。\n核心组件包括面向病毒进化的定制化蛋白质大语言模型（国产 AI 超算「鹏城云脑 II」256 张 NPU 支撑训练）、突变所处相互作用网络的全面重建模块（包含动态粒度注意力机制以挖掘 motif 模式）以及提出的多任务焦点损失函数。\n进化模式的精准捕获\n以SARS-CoV-2 的三类关键病毒进化驱动力预测任务为例，团队首先比较了面向进化场景的定制化蛋白质语言与主流蛋白质语言模型的预测表现。\n结果表明，团队定制化的蛋白质语言模型以最少的 340M 模型参数量实现了最佳的预测表现，甚至超越了参数量为其 44 倍的 ESM2-15B 的效果，这进一步证明了定制化的预训练数据集和训练策略的有效性。\n随后，团队在各类关键病毒进化驱动力预测任务下比较了 E2VD 与主流方法，结果表明 E2VD 显著且全面超越其他方法，性能提升在 7%-21% 不等。\nE2VD 被大量消融实验证明了对于病毒进化模式的精准捕获，包括对于不同类型突变的精准区分以及对稀少有益突变的精准挖掘。\n团队提出的多任务焦点损失函数被证明显著改善了预测表现，将 Accurate从57.41% 提升至 91.11%，将 Recall从15.56% 提升至 96.30%。\n在与真实世界变异毒株对应的稀少有益突变预测实验设置下，E2VD 将稀少有益突变的预测精度从 13% 提升至 80%，实现了跨越式精度提升。\n[图片: https://image.jiqizhixin.com/uploads/editor/95efd0ec-f434-489a-afb0-69fe0c17636a/640.png]\n图 2：E2VD 对于突变类型的区分和稀少有益突变的精准挖掘。（来源：论文）\n跨病毒类型和跨毒株的泛化性能\nE2VD 在跨越病毒类型和毒株类型时展现出强大的泛化能力。研究团队提出鲁棒且避免实验批次效应影响的突变所致病毒适应度变化评估指标，并以此评估了模型在同病毒类型的不同毒株之间以及不同病毒类型之间的泛化表现，在新冠病毒、寨卡病毒、流感病毒以及艾滋病病毒上展现出理想的泛化能力，始终超越其他方法，未来可进一步拓展至更多传染性病毒。\n[图片: https://image.jiqizhixin.com/uploads/editor/204b84f7-71a0-4ecf-9ba6-b49bba646341/640.png]\n图 3：E2VD 跨病毒类型和跨毒株的泛化性能。（来源：论文）\n多尺度进化趋势预测\nE2VD 可用于灵活定制化组合以实现不同尺度的进化趋势预测。首先，E2VD 可用于解释大流行内部进化轨迹，揭示毒株流行度背后隐藏的分子机制；其次，搭配虚拟深度突变扫描流程，E2VD 可实现潜在高风险突变的精准预测，达到 80% 的命中率。\n除此之外，E2VD 实现了对于大流行尺度的宏观进化轨迹预测，重现了病毒在真实世界中的进化路线，对病毒进化机制的解读提供理论性支撑。\n[图片: https://image.jiqizhixin.com/uploads/editor/46981bcc-aea2-48ae-ba23-540fd04fa5ae/640.png]\n图 4：E2VD 解释大流行内部进化轨迹以及预测潜在高风险突变。（来源：论文）\n总结与展望\n该研究以进化论的视角重新审视病毒进化预测问题，发展了跨病毒类型和跨毒株的通用进化预测框架，有助于破解物种复杂的进化机制，提高人类对于新发病毒感染的响应速度。凭借优越的预测表现和强大的泛化性，研究团队下一步计划将 E2VD 与疫苗和蛋白类药物设计流程相结合，以期提升设计效率和设计可控度。\n自 2022 年起，北京大学田永鸿教授领衔的团队即着眼于 AI for Life Science 的研究，发展系列生命科学基础模型并开展广泛的下游任务探索。\n前期工作提名 2022 年度戈登贝尔特别奖，与美国阿贡国家实验室、橡树岭国家实验室团队在世界舞台上角逐这一超级计算机领域的国际最高奖项，展现了中国人工智能在计算集群（国产 AI 超算鹏城云脑 II）和科研创新领域的国际顶尖水平。\n除此之外，团队先后获得 2023 年度广东省科学技术奖科技进步奖特等奖、首届「祖冲之奖——人工智能前沿创新奖年度重大成果奖」以及国家数据局 2024 年「数据要素×」大赛广东省一等奖、全国二等奖等荣誉。\n论文链接：\nhttps://www.nature.com/articles/s42256-024-00966-9\n入围戈登贝尔特别奖新闻链接：\nhttps://news.pku.edu.cn/jxky/90d276ae5f8441849fd04372fd872154.htm",
    "tags": [
      "产业",
      "产业"
    ],
    "category": "产业",
    "images": [
      "https://image.jiqizhixin.com/uploads/editor/ef42d330-06d5-4d54-894f-82c2458dd3e1/640.png",
      "https://image.jiqizhixin.com/uploads/editor/001269ea-e7d6-44eb-b9e3-6afc9f30015a/640.png",
      "https://image.jiqizhixin.com/uploads/editor/643ec236-726a-482f-bfdb-67bd0ef8f8f3/640.png",
      "https://image.jiqizhixin.com/uploads/editor/95efd0ec-f434-489a-afb0-69fe0c17636a/640.png",
      "https://image.jiqizhixin.com/uploads/editor/204b84f7-71a0-4ecf-9ba6-b49bba646341/640.png",
      "https://image.jiqizhixin.com/uploads/editor/46981bcc-aea2-48ae-ba23-540fd04fa5ae/640.png"
    ]
  },
  {
    "url": "https://www.jiqizhixin.com/articles/2025-01-21-3",
    "title": "清北团队进军具身智能，银河通用、灵初智能、星海图齐发力",
    "author": "新闻助手原创2025/01/21 11:00",
    "date": "2025/01/21 11:00",
    "content": "具身智能创业如火如荼，技术路线是否收敛、以及数据来源的选择，都是大家一直关心的问题。最近清华北大的团队密集发布了很多研究成果，我们或许可以从中分析出一些趋势。\n[图片: https://image.jiqizhixin.com/uploads/editor/7239578f-00ee-4366-89d0-ed6ddadaf049/1.jpg]\n23 年初成立的银河通用背后是前如布科技联创尹方鸣和姚腾洲、科学家是北大助理教授王鹤。银河通用是低成本仿真路线的拥护者，经过 2 年努力于近期重磅发布了 GraspVLA，思路与 RoboCasa、RoboGen 等类似，在海量合成的仿真环境中合成机器人数据。但 GraspVLA 只关注抓取任务，将预训练的 AnyGrasp 模型部署到仿真中采集大量数据来训练一个 VLA。在仿真中可以加入很多随机化、以提升 VLA 的泛化性。\n[图片: https://image.jiqizhixin.com/uploads/editor/d0a24cde-3433-4d34-b7f3-77d402967d88/image004.gif]\n[图片: https://image.jiqizhixin.com/uploads/editor/e24b0d3d-be83-426c-aa81-fc2b87eb740a/image005.gif]\n[图片: https://image.jiqizhixin.com/uploads/editor/d31431f1-b289-4d5c-ab8f-29ca8c004b4d/image006.gif]\nAnyGrasp、GraspVLA、OpenVLA demo视频对比\n2024 年 9 月成立的灵初智能，CEO 是前京东机器人总裁王启斌、以及机器人算法负责人柴晓杰、李飞飞学生陈源培，背后科学家包括北大助理教授杨耀东和梁一韬。\n[图片: https://image.jiqizhixin.com/uploads/editor/4813d8d5-aa2c-4f24-91cf-c26bbb35c2de/image007.gif]\nPsi R0 的 demo 视频\n与银河通用类似，灵初智能也是在仿真环境中大规模预训练模型，但在模仿学习中加入了强化学习技术、以及真机数据对齐微调训练，使得即使只用少量仿真和真机数据也能做到很泛化的复杂任务，实现不同技能顺滑串联操作。2024 年 12 月底发布的 Psi R0 模型完成了双手协作长程的泛化打包任务，已展现出了该模型能实现真正商业化的强大潜力。\n灵初智能此前的其他成果，比如 lego 组装也是长程的灵巧手任务，可以突破过去强力抓取的能力边界、完成更灵活的抓取和灵巧动作。根据之前的公开信息，灵初智能将于 3 月份发布自研本体以及更泛化的具身大模型。\n[图片: https://image.jiqizhixin.com/uploads/editor/12b39cb9-f061-4ed6-bc0a-6278b36be35e/image008.gif]\n以上为 Lego 组装视频\n在数据选择方面，23 年 9 月成立的清华系星海图持完全不同的观点，他们认为数据价值上，真机数据 > 互联网数据 > 仿真数据。星海图 CEO 是 Momenta 前执行董事高继扬，科学家包括清华助理教授赵行和许华哲。他们计划今年发布 100 万条真机数据、明年发布 1000 万条真机数据。\n星海图计划采用真机数据为主来预训练具身大模型、而不是灵初和银河那种大规模仿真数据预训练。但以大规模真机数据为主存在 diverse 不足的问题，无法涌现泛化。\n[图片: https://image.jiqizhixin.com/uploads/editor/3c6e52b1-1d85-4480-8425-672cb3d90de4/image009.gif]\n以上为星海图 real2sim2real 视频 demo\n在仿真数据方面，星海图强调 Real2Sim2Real 后训练。仿真数据只作为后训练的一个强化剂，将真实数据在仿真中加入随机化来扩充 1000 倍，以实现更高的成功率和更好的落地效果。\n[图片: https://image.jiqizhixin.com/uploads/editor/76a8bd4c-31a5-4ce5-84e7-d4616134288b/3.jpg]\n三家清北团队在算法和数据选择上略有不同。灵初智能在算法上强调强化学习、银河在数据上强调仿真、星海图强调真实数据。不过各家都采用了仿真和真实数据结合的方法，只是在预训练和后训练上强调不同的数据比例。\n期待这几家准独角兽公司在未来带来更多的惊喜。清华北大是具身智能创新的先锋，近期还有很多有意思的成果。比如清华星动纪元 ERA-42、北大与国地共建具身智能中心 RoboMind、北大与智元 OmniManip、清华千寻智能 CoPa 和 Data Scaling Law 等工作都很值得分析。",
    "tags": [
      "产业清华大学创业公司机器人技术具身智能",
      "产业",
      "清华大学",
      "创业公司",
      "机器人技术",
      "具身智能"
    ],
    "category": "产业",
    "images": [
      "https://image.jiqizhixin.com/uploads/editor/7239578f-00ee-4366-89d0-ed6ddadaf049/1.jpg",
      "https://image.jiqizhixin.com/uploads/editor/d0a24cde-3433-4d34-b7f3-77d402967d88/image004.gif",
      "https://image.jiqizhixin.com/uploads/editor/e24b0d3d-be83-426c-aa81-fc2b87eb740a/image005.gif",
      "https://image.jiqizhixin.com/uploads/editor/d31431f1-b289-4d5c-ab8f-29ca8c004b4d/image006.gif",
      "https://image.jiqizhixin.com/uploads/editor/4813d8d5-aa2c-4f24-91cf-c26bbb35c2de/image007.gif",
      "https://image.jiqizhixin.com/uploads/editor/12b39cb9-f061-4ed6-bc0a-6278b36be35e/image008.gif",
      "https://image.jiqizhixin.com/uploads/editor/3c6e52b1-1d85-4480-8425-672cb3d90de4/image009.gif",
      "https://image.jiqizhixin.com/uploads/editor/76a8bd4c-31a5-4ce5-84e7-d4616134288b/3.jpg"
    ]
  }
]